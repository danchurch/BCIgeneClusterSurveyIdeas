## let's take a look at the BCI tree census data

## let's see if we can a geopandas df going...

import pandas as pd
import shapely as sh
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import rasterio as rs
import rasterio.plot as rsPlot
import Bio
from Bio import Entrez
plt.ion()

## think this is a tsv:

censusPath = '/home/daniel/Documents/job_apps/panama/data/treeCensusData/PlotDataReport10-13-2020_1762275259.txt'
BCItrees = pd.read_csv(censusPath, sep='\t')

BCItrees.head()

(BCItrees.Census == 7).all()

## spatial data is in there, can we make that into a shapely points obj?

BCIgdf = gpd.GeoDataFrame(
    BCItrees, geometry=gpd.points_from_xy(BCItrees.PX, BCItrees.PY))

## how does that look?
BCIgdf.head()
 
## great. Now what do we want?

## we basically want a rank abundance curve...

## we need to group by species, sum, and rank

aa = BCIgdf.groupby(BCIgdf['Latin'])
aa.agg(len) ## works. but a better way?
aa.agg(np.size) ## also works. same, basically
abundances = aa.agg(np.size)['Census'].copy() ## not that clever, but works.
abundances.sort_values(ascending=False, inplace=True)


## that worked...? sanity checks
## there should be 10631 lines with Alseis blackiana
(BCItrees.Latin == 'Alseis blackiana').sum() ## yup
## there should be 10631 lines with Alseis blackiana
abundances.tail()
## Pterocarpus officinalis should be observed only once
(BCItrees.Latin == 'Pterocarpus officinalis').sum() ## yup
np.sum(abundances)
BCIgdf.shape 

## rank abundence plot:

plt.close('all')
xs = np.arange(0,len(abundances))
plt.bar(xs, abundances)
## give names:
plt.bar(xs, abundances)
plt.gca().set_xticks(xs)
plt.gca().set_xticklabels(abundances.index, rotation=90)
plt.gca().set_xlim(-0.5,10.5)
plt.tight_layout()

## zoom in
plt.gca().set_xlim(0,10)




##### generating map ##############

## can we read in the nice hillshade tiff of the island?

tifPath = '/home/daniel/Documents/job_apps/panama/data/GIS/BCI_ColoredShaded_Relief/BCI_ColoredShaded_Relief.tif'
twfPath = '/home/daniel/Documents/job_apps/panama/data/GIS/BCI_ColoredShaded_Relief/BCI_ColoredShaded_Relief.tif'
BCIrast = rs.open(tifPath)

aa = BCIrast.read()

help(rs.open)

rsPlot.show(BCIrast)
## that didn't work...

plt.imshow(BCIrast)
## that either 


BCIrast = rs.open(tifPath, GEOREF_SOURCES='INTERNAL' )


###################
## not working. Some folks are saying try gdal, in BASH:
gdal_translate -of GTiff BCI_ColoredShaded_Relief.tif BCI_georeff.tiff
## that worked, fixed the above.
###################

tifPath = '/home/daniel/Documents/job_apps/panama/data/GIS/BCI_ColoredShaded_Relief/BCI_georeff.tiff'
BCIrast = rs.open(tifPath)
rsPlot.show(BCIrast)

## that works. 
## now can we add the points of a tree that we are interested in?

abundances.head()

## I like rubiaceae, so...

aa = BCIgdf.groupby(BCIgdf['Latin'])
faramea = aa.get_group('Faramea occidentalis')

faramea.shape

plt.close('all')

fig, ax = plt.subplots(1,1)

tifPath = '/home/daniel/Documents/job_apps/panama/data/GIS/BCI_ColoredShaded_Relief/BCI_georeff.tiff'
BCIrast = rs.open(tifPath)

ax=plt.gca()

rsPlot.show(BCIrast)

rsPlot.show(BCIrast)
faramea.plot(color='red', ax=ax)

## oops, that's not gonna work until figure out where the plot is, UTM-wise
## we have this geojson:

fig, ax = plt.subplots(1,1)

BCI2020path='/home/daniel/Documents/job_apps/panama/data/GIS/Barro_Colorado_Island_20mx20m_Quadrants_-_Feature_Layer.geojson'
BCI2020 = gpd.read_file(BCI2020path)

BCI2020.crs
## this is in lat/long 
## reproject:
## epsg code for 17n is 32617

BCI2020.to_crs("EPSG:32617", inplace=True)

rsPlot.show(BCIrast)
ax = plt.gca()
BCI2020.plot(ax=ax)

## so how can we use this to plot our points?

## we have this point from the website:
## Latitude: 9.154300000000 Longitude: -79.846100000000 
## is this the origin of our plot?

origLL = sh.geometry.Point(-79.8461,9.1543)
d = {'number':['zoop'], 'geometry':[origLL]}
origDF = gpd.GeoDataFrame( d, crs="epsg:4326" )
origDF.to_crs("EPSG:32617", inplace=True)

plt.close('all')

BCI2020.plot()
ax=plt.gca()
origDF.plot(ax=ax)

## works. nope, that is not the origin, unless there are projection
## issues here. 

## anyway, this does not have to be perfect. Let's start by guessing the 
## origin from the quadrant graphic we have:

xa, ya = 625774, 1.01178*10**6
## now can we convert our faramea points with this?
faramea.set_geometry(gpd.points_from_xy(faramea.PX + xa, faramea.PY + ya), inplace=True)

faramea.plot()

## start over

import pandas as pd
import shapely as sh
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import rasterio as rs
import rasterio.plot as rsPlot
import Bio.Seq 
import os, re, copy
from matplotlib.patches import Rectangle as rect
from Bio.Blast import NCBIWWW
from Bio import Entrez
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
from Bio.Align.Applications import MuscleCommandline
Entrez.email = "danchurchthomas@gmail.com"
plt.ion()

## get data
tifPath = '/home/daniel/Documents/job_apps/panama/data/GIS/BCI_ColoredShaded_Relief/BCI_georeff.tiff'
BCIrast = rs.open(tifPath)
BCI2020path='/home/daniel/Documents/job_apps/panama/data/GIS/Barro_Colorado_Island_20mx20m_Quadrants_-_Feature_Layer.geojson'
BCI2020 = gpd.read_file(BCI2020path)
BCI2020.to_crs("EPSG:32617", inplace=True)
censusPath = '/home/daniel/Documents/job_apps/panama/data/treeCensusData/PlotDataReport10-13-2020_1762275259.txt'
BCItrees = pd.read_csv(censusPath, sep='\t')
BCIgdf = gpd.GeoDataFrame(
    BCItrees, geometry=gpd.points_from_xy(BCItrees.PX, BCItrees.PY))
streamPath = '/home/daniel/Documents/job_apps/panama/data/GIS/BCI_streams.geojson'
BCIstreams = gpd.read_file(streamPath)
## sample grid
xa, ya = 625774, 1.01178*10**6
sampleX, sampleY = np.meshgrid([ i*100+xa for i in range(1,10) ],
                                [ i*100+ya for i in range(1,5) ])


## this paper has publically available transcriptomic data for six species. 
https://doi.org/10.1111/mec.13999
## these species have been studied quite a bit...maybe that will strengthen the case

## what are they?

Beilschmiedia pendula

Virola surinamensis 
Brosimum alicastrum 
Eugenia nesiotica 
Lacmellea panamensis 
Dipteryx oleifera 

"Eugenia nesiotica"

## can we find these?

## make a function:


def showSpMap(sp, colr):
    plt.close('all')
    aa = BCIgdf.groupby(BCIgdf['Latin'])
    spDF = aa.get_group(sp).copy()
    xa, ya = 625774, 1.01178*10**6
    spDF.set_geometry(gpd.points_from_xy(spDF.PX + xa, spDF.PY + ya), inplace=True)
    rsPlot.show(BCIrast)
    ax = plt.gca()
    BCIstreams.plot(ax=ax, color="blue")
    spDF.plot(ax=ax, color=colr, zorder=3)
    ax.plot(sampleX, sampleY, 
                marker='s',
                markersize=10,
                markeredgewidth=2,
                markeredgecolor='red',
                linestyle='none',
                fillstyle='none',
                color='black',
                zorder=4,
                #markerfacecolor='black',
                )
    ax.set_xlim(spDF.geometry.x.min()-50,spDF.geometry.x.max()+50)
    ax.set_ylim(spDF.geometry.y.min()-50,spDF.geometry.y.max()+50)
    ax.vlines(x=[xa,xa+1000], ymin=ya, ymax=ya+500)
    ax.hlines(xmin=xa, xmax=xa+1000, y=[ya, ya+500])
    #plt.gcf().suptitle(sp)
    plt.tight_layout()
    return(ax)

sp = "Eugenia nesiotica"
colr = 'green'
aa = showSpMap(sp, colr)



## how do we get our sampling scheme on there?

## the thought is to do a 4 x 9 matrix. Corner would be (xa+100, ya+100), 
## then every 100m in each direction:

list(range(1,10))


sampleX, sampleY = np.meshgrid([ i*100+xa for i in range(1,10) ],
                                [ i*100+ya for i in range(1,5) ])
plt.scatter(xx,yy, 
            color="black",
            marker="^",
)

xx, yy = np.meshgrid(sampleX, sampleY, sparse=True)

gpd.points_from_xy(sampleX, sampleY)

xsample = xa + 



plt.gca()


aa = gpd.points_from_xy(x=[xa, xa+1000, xa+1000, xa], y=[ya, ya, ya+1000, ya+1000])

plt.gca().add_patch(bb)


aa = [
"Beilschmiedia pendula",
"Brosimum alicastrum",
"Dipteryx oleifera",
"Eugenia nesiotica",
"Lacmellea panamensis",
"Virola surinamensis", 
]


plt.close('all')

showSpMap(aa[0], 'blue')

showSpMap(aa[1], 'green')

showSpMap(aa[2], 'red')

showSpMap(aa[3], 'black')

showSpMap(aa[4], 'brown')

showSpMap("Eugenia nesiotica", 'black')

## huh. I think any of these will do...one of them was not covered ?
## these were the ones covered by the Mangan et al 2010 study:

Brosimum alicastrum
Beilschmiedia pendula
Eugenia nesiotica
Lacmellea panamensis
Tetragastris panamensis
Virola surinamensis

## looks like dipteryx is not here, instead we have Tetragastris..

## so our options look like this:

aa = [
"Beilschmiedia pendula",
"Brosimum alicastrum",
"Eugenia nesiotica",
"Lacmellea panamensis",
"Virola surinamensis", 
]

plt.close('all')

showSpMap(aa[0], 'blue')

showSpMap(aa[1], 'green')

showSpMap(aa[2], 'red')

showSpMap(aa[3], 'black')

showSpMap(aa[4], 'brown')

## any of these have genomes?
## Eugenia has one congeneric, E. uniflora. I think this is the brazilian cherry 
## says there is a genome, but its only 3mb, can't be right. 
## oh, there it is: 0.03x coverage. That's like zero coverage, but not  quite. 

## okay, so stay with Eugenia for the moment...

## what does a map for this look like?


showSpMap("Eugenia nesiotica", 'black')

## we want a bounding box, and a sampling grid. 

## bounding box:


#########################################################################

## work back a little...how long are these primers?

## we need to know this, to see if illumina sequencing is possible with them...

## then we can decide how many samples we need to achieve a mininum sequencing 
## depth...


## okay, so how do we get some sample NRPS and PKS ? 

## for the NRPS enzyme, they use 
A3F (5′-GCSTACSYSATSTACACSTCSGG)
A7R (5′-SASGTCVCCSGTSCGGTA) 
## from (Ayuso-Sacido and Genilloud, 2005)

## and for PKS they use:
KS (5′-GCIATGGAYCCICARCARMGIVT) 
degKS2R.i (5′-GTICCIGTICCRTGISCYTCIAC)
## from (Schirmer et al., 2005), sponge paper.

## the Ayuso-Sacido paper has PKS type I primers, wonder why they didn't use these?
## dunno. let's just follow their lead for the moment...

## what next...we need some NRPS sequences to look at...

## how can we get these? Seems like a job for genbank and biopython

########## get some data from genbank ########

## for starters, let's check ~100 sequences of NRPS adenylation domain
## and see how these primers behave?

Entrez.email = "danchurchthomas@gmail.com"

## what do we need here? We need a set of NRPS to play with. How do we get this?
allNRPShandle = Entrez.esearch(db="nucleotide", 
    term="non-ribosomal peptide synthetase adenylation domain", 
#    retmax=7000,
    idtype="acc",
)
allNRPSrecord = Entrez.read(allNRPShandle)
allNRPSrecord['Count'] ## 6607
allNRPSrecord['IdList']


## how can we subset to just fungi?
justFungiNRPShandle = Entrez.esearch(db="nucleotide", 
    term="non-ribosomal peptide synthetase adenylation domain AND fungi [ORGN]", 
    idtype="acc",
    retmax=50,
)
justFungiNRPSrecord = Entrez.read(justFungiNRPShandle)
justFungiNRPSrecord['Count'] ## far fewer, 54

justFungiNRPSrecord['IdList'] 

justFungiNRPSrecord


## okay, now bacteria:
justBacteriaNRPShandle = Entrez.esearch(db='nucleotide',
    term="non-ribosomal peptide synthetase adenylation domain AND Bacteria [ORGN]", 
    idtype="acc",
    retmax=50,
)
justBacteriaNRPSrecord = Entrez.read(justBacteriaNRPShandle)
justBacteriaNRPSrecord['Count'] ## lots, 6544
justBacteriaNRPSrecord['IdList']

## what is the difference here between nucleotide and gene?
justFungiNRPShandle = Entrez.esearch(db="gene", 
    term="non-ribosomal peptide synthetase adenylation domain AND fungi [ORGN]", 
    idtype="acc",
    retmax=100,
)
justFungiNRPSrecord = Entrez.read(justFungiNRPShandle)
justFungiNRPSrecord['Count'] ## huh, that's a lot more...538
justFungiNRPSrecord['IdList'] ## these are not acession numbers...

justBacteriaNRPShandle = Entrez.esearch(db="gene", 
    term="non-ribosomal peptide synthetase adenylation domain AND bacteria [ORGN]", 
    idtype="acc",
    retmax=100,
)
justBacteriaNRPSrecord = Entrez.read(justBacteriaNRPShandle)

justBacteriaNRPSrecord['Count'] ## dramatically less than the nucleotide database 1175

## for kicks, look at plants?
justPlantsNRPShandle = Entrez.esearch(db="gene", 
    term="(non-ribosomal peptide synthetase adenylation domain) AND green plants[porgn:__txid33090]", 
    idtype="acc",
    retmax=100,
)
justPlantsNRPSrecord = Entrez.read(justPlantsNRPShandle)
justPlantsNRPSrecord['Count'] ## just 3


## 'gene' may be just genbank results, the hopefully well-curated gene sequences,
## excluding the annotated chromosomes with automated matches etc. 
## might be better to use...let's keep them...
## we might consider randomizing our sample a bit. But I assumed there is a bit
## of quality control built into the search algorithmns, in that the first 
## matches are more likely to actually be NRPSes than later matches?
## so keep these top 100 matches of each for now...

## these can be downloaded from genbank?

## for one record:
handle = Entrez.efetch(db="gene", id="EU490707", rettype="gb", retmode="fasta")

help(Entrez.esearch)
`
help(Entrez.efetch)

## says ids can be comma separated lists, can these be literal lists in python?

aa = justPlantsNRPSrecord['IdList'][0:3]

aa = justFungiNRPSrecord['IdList'][0:3]

handle = Entrez.efetch(db="gene", id=aa, rettype="gb", retmode="fasta")

handle = Entrez.efetch(db="gene", id=aa, rettype="gb", retmode="text")

print(handle.read())

## this is working but not returning sequence data. Also, many don't seem to 
## be NRPSes, even fungal. Think we need to use the nucleotide db to get this?
## not sure if these IDs I have are unique

bb = justPlantsNRPSrecord['IdList'][0]

handle = Entrez.efetch(db="nucleotide", id=bb, rettype="gb", retmode="text")
## is this the same gene as?
handle2 = Entrez.efetch(db="gene", id=bb, rettype="gb", retmode="text")
## nope. 
## I think we need accession numbers

## for one record:

justFungiNRPShandle = Entrez.esearch(db="gene", 
    term="non-ribosomal peptide synthetase adenylation domain AND fungi [ORGN]", 
    idtype="acc",
    retmax=10,
    retmode='text',
)

justFungiNRPSrecord = Entrez.read(justFungiNRPShandle)

justFungiNRPSrecord

print(handle2.read())

## a classic efetch goes like this:
handle = Entrez.efetch(db="nucleotide", id="EU490707", rettype="gb", retmode="text")

## you start with an accession number. And dammit, esearch doesn't seem to return 
## accession numbers. 
## and NCBI is dropping the "GI" number system, which is what esearch returns. 
## as usual, the API for NCBI is a mess.  

## this is not going well...

## okay, so maybe we can hope that they haven't totally phased out the 
## GI numbers, and pipe our esearch results into an efetch  

## or we can use the nucleotide results...:

allNRPSrecord['IdList']

## how can we download these? I think it would look like this:

aa = Entrez.efetch(db="nucleotide", id=allNRPSrecord['IdList'], rettype="gb", retmode="text")

## but let's wait to do that. Apparently there is a better way, by directly caching and piping
## esearch results to efetch, detailed here:
## http://biopython.org/DIST/docs/tutorial/Tutorial.html#sec%3Aentrez-webenv

## so maybe this could be used for the gene database:
allNRPShandle = Entrez.esearch(db="gene", 
    term="non-ribosomal peptide synthetase adenylation domain", 
    retmax=7000,
    idtype="acc",
    usehistory="y",
)

## gives us the usual info:
allNRPSrecord = Entrez.read(allNRPShandle)
allNRPSrecord['Count'] ## 6607
allNRPSrecord['IdList']

acc_list = allNRPSrecord["IdList"]
count = int(allNRPSrecord["Count"])

count == len(acc_list)

## but also the additional variables:

webenv = allNRPSrecord["WebEnv"]
query_key = allNRPSrecord["QueryKey"]

## try getting a few of these with efetch:

fetch_handle = Entrez.efetch(
    db="gene",
    rettype="fasta",
    retmode="text",
    retmax=10,
    webenv=webenv,
    query_key=query_key,
    idtype="acc",
)

data = fetch_handle.read()

fetch_handle.close()

with open('allNRPS.txt', 'w') as f:
    f.write(data)

## That's promising. But no sequence data.

## the genes don't seem to return any sequence data
## I guess the gene database is more focused on 
## collating information about the role of the gene
## and the various sequences associated with it,
## genomic, mRNA etc.

## seems like we're back to the nucleotide database here:

## do one of adenylation domains:
allNRPSADhandle = Entrez.esearch(db="nucleotide", 
    term="non-ribosomal peptide synthetase adenylation domain", 
    retmax=7000,
    idtype="acc",
    usehistory="y",
)
allNRPSADrecord = Entrez.read(allNRPSADhandle)
acc_list = allNRPSADrecord["IdList"]
count = int(allNRPSADrecord["Count"])
webenvAD = allNRPSADrecord["WebEnv"]
query_keyAD = allNRPSADrecord["QueryKey"]
fetchAD = Entrez.efetch(
    db="nucleotide",
    rettype="fasta",
    retmode="text",
    retmax=10,
    webenv=webenvAD,
    query_key=query_keyAD,
    idtype="acc",
)
dataAD = fetchAD.read()
with open("ADfastas.txt", 'w') as f:
    f.write(dataAD)
fetchAD.close()

## that looks good. how different does a less specific NRPS (no AD) 
## search look? 

allNRPShandle = Entrez.esearch(db="nucleotide", 
    term="non-ribosomal peptide synthetase", 
    retmax=7000,
    idtype="acc",
    usehistory="y",
)

allNRPSrecord = Entrez.read(allNRPShandle)
acc_list = allNRPSrecord["IdList"]
count = int(allNRPSrecord["Count"])
webenv = allNRPSrecord["WebEnv"]
query_key = allNRPSrecord["QueryKey"]
fetch = Entrez.efetch(
    db="nucleotide",
    rettype="fasta",
    retmode="text",
    retmax=10,
    webenv=webenv,
    query_key=query_key,
    idtype="acc",
)

dataNRPS = fetch.read()
with open("NRPSfastas.txt", 'w') as f:
    f.write(dataNRPS)

fetch.close()


## problem, these don't divide by kingdom, which we will need for the 
## writeup (one for fungi, one for bacteria, one for plants)

## so new plan, get these individually by kingdom:

justFungiNRPShandle = Entrez.esearch(db="nucleotide", 
    term="non-ribosomal peptide synthetase AND fungi [ORGN]", 
    idtype="acc",
    retmax=2500,
)
justFungiNRPSrecord = Entrez.read(justFungiNRPShandle)
justFungiNRPSrecord['Count'] ## 2234 

## bacteria - there are so many, let's take the first 50000
justBactNRPShandle = Entrez.esearch(db="nucleotide", 
    term="non-ribosomal peptide synthetase AND bacteria [ORGN]", 
    idtype="acc",
    retmax=50000,
)
justBactNRPSrecord = Entrez.read(justBactNRPShandle)
justBactNRPSrecord['Count'] ## 143457

justPlantNRPShandle = Entrez.esearch(db="nucleotide", 
    term="non-ribosomal peptide synthetase AND green plants[porgn:__txid33090]", 
    idtype="acc",
    retmax=100,
)
justPlantNRPSrecord = Entrez.read(justPlantNRPShandle)
justPlantNRPSrecord['Count'] ## 55
justPlantNRPSrecord['IdList']
## I'll need these to see if our primers pick up these 
## hopefully they don't 

## are our AD results included in these?

pd.Series(allNRPSADrecord["IdList"]).isin(pd.Series(justBactNRPSrecord['IdList']))
pd.Series(allNRPSADrecord["IdList"]).isin(pd.Series(justBactNRPSrecord['IdList'])).sum()
pd.Series(allNRPSADrecord["IdList"]).isin(pd.Series(justBactNRPSrecord['IdList']))
pd.Series(allNRPSADrecord["IdList"]).isin(pd.Series(justBactNRPSrecord['IdList'])).sum() 
## about a third. makes sense, we downloaded about a third of the bacterial NRPS

pd.Series(allNRPSADrecord["IdList"]).isin(pd.Series(justBactNRPSrecord['IdList'])).any()
pd.Series(allNRPSADrecord["IdList"]).isin(pd.Series(justBactNRPSrecord['IdList'])).all()

allNRPSADrecord["IdList"] 


[ i in justBactNRPSrecord['IdList'] for i in allNRPSADrecord["IdList"] ]

[ i in allNRPSADrecord["IdList"] for i in justBactNRPSrecord['IdList'] ]

justBactNRPSrecord['IdList']

allNRPSADrecord["IdList"] 

## okay, so set up the downloads, one for each group. try the small ones first:

## plants:
justPlantNRPShandle = Entrez.esearch(db="nucleotide", 
    term="non-ribosomal peptide synthetase AND green plants[porgn:__txid33090]", 
    idtype="acc",
    retmax=100,
    usehistory="y",
)

justPlantNRPSrecord = Entrez.read(justPlantNRPShandle)


acc_list = justPlantNRPSrecord["IdList"]
count = int(justPlantNRPSrecord["Count"])
webenv = justPlantNRPSrecord["WebEnv"]
query_key = justPlantNRPSrecord["QueryKey"]

fetch = Entrez.efetch(
    db="nucleotide",
    rettype="fasta",
    retmode="text",
    retmax=len(acc_list),
    webenv=webenv,
    query_key=query_key,
    idtype="acc",
)

dataNRPS = fetch.read()
with open("PlantNRPSfastas.txt", 'w') as f:
    f.write(dataNRPS)

fetch.close()

## and that results in a big damn file. just 55 sequences.
## we are not going to be able to do this locally.

## How about we build a small library of 100 sequences of bacteria and fungi each, 
## play around with that?

justBactNRPShandle = Entrez.esearch(db="nucleotide", 
    term="non-ribosomal peptide synthetase AND bacteria [ORGN]", 
    idtype="acc",
    retmax=100,
    usehistory="y",
)

justBactNRPSrecord = Entrez.read(justBactNRPShandle)

webenv = justBactNRPSrecord["WebEnv"]
query_key = justBactNRPSrecord["QueryKey"]
fetch = Entrez.efetch(
    db="nucleotide",
    rettype="fasta",
    retmode="text",
    retmax=len(justBactNRPSrecord["IdList"]),
    webenv=webenv,
    query_key=query_key,
    idtype="acc",
)

dataNRPS = fetch.read()
with open("BactAndFungiNRPSfastas.txt", 'w') as f:
    f.write(dataNRPS)

fetch.close()

justFungiNRPShandle = Entrez.esearch(db="nucleotide", 
    term="non-ribosomal peptide synthetase AND fungi [ORGN]", 
    idtype="acc",
    retmax=100,
    usehistory="y",
)

justFungiNRPSrecord = Entrez.read(justFungiNRPShandle)
webenv = justFungiNRPSrecord["WebEnv"]
query_key = justFungiNRPSrecord["QueryKey"]

fetch = Entrez.efetch(
    db="nucleotide",
    rettype="fasta",
    retmode="text",
    retmax=len(justFungiNRPSrecord["IdList"]),
    webenv=webenv,
    query_key=query_key,
    idtype="acc",
)

dataNRPS = fetch.read()

with open("BactAndFungiNRPSfastas.txt", 'a') as f:
    f.write(dataNRPS)

fetch.close()

## make a local blastdb, search that for our primers
## in BASH

makeblastdb -in BactandFungi.fa -parse_seqids -dbtype prot

## look for our primers:

A3F=GCSTACSYSATSTACACSTCSGG
A7R=SASGTCVCCSGTSCGGTA
echo '> A3F' > 'nrpsPrimers.fa' 
echo $A3F >> 'nrpsPrimers.fa'
echo '> A7R' >> 'nrpsPrimers.fa'
echo $A7R >> 'nrpsPrimers.fa'

cat nrpsPrimers.fa

blastn -query nrpsPrimers.fa -subject BactandFungi.fa -evalue 30000

## reverse comp of A3F:
CC.GA.GTGTA.AT...GTA.G

grep GC.TAC...AT.TACAC.TC.GG BactandFungi.fa 

grep GC.TAC...AT.TACAC.TC.GG BactandFungi.fa | wc -l

grep CC.GA.GTGTA.AT...GTA.G BactandFungi.fa 

grep CC.GA.GTGTA.AT...GTA.G BactandFungi.fa  | wc -l

## 30 diff hits...
## they are in there. why doesn't blast pick them up?

## try with the Ns instead of the other ambiguous symbols: 

A3F=GCNTACNNNATNTACACNTCNGG
A7R=SASGTCVCCSGTSCGGTA
echo '> A3F' > 'nrpsPrimers.fa' 
echo $A3F >> 'nrpsPrimers.fa'
echo '> A7R' >> 'nrpsPrimers.fa'
echo $A7R >> 'nrpsPrimers.fa'
blastn -query nrpsPrimers.fa -subject BactandFungi.fa -evalue 30000

## nada...

## let's align them, and look at them. using muscle:

muscle -in BactandFungi.fa -out BactandFungiAligned.fa

## failed. No diagnostics as to why.

## sleep. Then try this with the AD domains...

## awake: Let's get the AD domain info, start an alignment:

## we want the same, ~100 of the bacterial, and as many fungal and
## plant homologues as we can get:

## plants
justPlantNRPShandle = Entrez.esearch(db="nucleotide", 
    term="non-ribosomal peptide synthetase AND green plants[porgn:__txid33090]", 
    idtype="acc",
    retmax=100,
    usehistory="y",
)

record = Entrez.read(justPlantNRPShandle)
webenv = record["WebEnv"]
query_key = record["QueryKey"]
fetch = Entrez.efetch(
    db="nucleotide",
    rettype="fasta",
    retmode="text",
    retmax=len(record["IdList"]),
    webenv=webenv,
    query_key=query_key,
    idtype="acc",
)

dataNRPS = fetch.read()
with open("B_F_P_NRPS_AD_fastas.txt", 'w') as f:
    f.write(dataNRPS)
    fetch.close()

## add bacteria
justBactNRPShandle = Entrez.esearch(db="nucleotide", 
    term="non-ribosomal peptide synthetase AND bacteria [ORGN]",
    idtype="acc",
    retmax=100,
    usehistory="y",
)

record = Entrez.read(justBactNRPShandle)
webenv = record["WebEnv"]
query_key = record["QueryKey"]

fetch = Entrez.efetch(
    db="nucleotide",
    rettype="fasta",
    retmode="text",
    retmax=len(record["IdList"]),
    webenv=webenv,
    query_key=query_key,
    idtype="acc",
)

dataNRPS = fetch.read()
with open("B_F_P_NRPS_AD_fastas.txt", 'a') as f:
    f.write(dataNRPS)
    fetch.close()

## add fungi
justFungiNRPShandle = Entrez.esearch(db="nucleotide", 
    term="non-ribosomal peptide synthetase AND fungi [ORGN]", 
    idtype="acc",
    retmax=100,
    usehistory="y",
)

record = Entrez.read(justFungiNRPShandle)
webenv = record["WebEnv"]
query_key = record["QueryKey"]

fetch = Entrez.efetch(
    db="nucleotide",
    rettype="fasta",
    retmode="text",
    retmax=len(record["IdList"]),
    webenv=webenv,
    query_key=query_key,
    idtype="acc",
)

dataNRPS = fetch.read()
with open("B_F_P_NRPS_AD_fastas.txt", 'a') as f:
    f.write(dataNRPS)
    fetch.close()

## okay..255 sequences...are these easier to align for MUSCLE?

mv B_F_P_NRPS_AD_fastas.txt B_F_P_NRPS_AD.fa

muscle -in B_F_P_NRPS_AD.fa -out BactandFungi_aligned.afa -maxiters 2

## these are all failing. If this continues, reduce the size. 

## keeps "killing" the process, even when I make through an iteration. 

## just to see, what happens with a tiny dataset?

## first ten reads of this fasta? How to subset in biopython...

ADparse = SeqIO.parse("B_F_P_NRPS_AD.fa", "fasta")

AdparseL = list(ADparse)

del(AdparseL)

## first we'll see if we just need to reduce the size of the alignment:

with open ('smallerFasta4Alignment.fa', "w") as f:
    SeqIO.write(AdparseL[0:101], f, "fasta")

## I think this excludes the fungi? so probably also makes the alignment easier


## try again with alignment

muscle -in smallerFasta4Alignment.fa  -out BactandFungi_aligned.afa -maxiters 2

## while that is running, I think a better approach here is generate alignments 
## for each of the three groups, fungi, plants, and bacteria. 

## here we go again...

## fungi
justFungiNRPShandle = Entrez.esearch(db="nucleotide", 
    term="(non-ribosomal peptide synthetase) AND Fungi[Organism] AND biomol_mRNA[Properties]",
    idtype="acc",
    retmax=20,
    usehistory="y",
)

record = Entrez.read(justFungiNRPShandle)
webenv = record["WebEnv"]
query_key = record["QueryKey"]
fetch = Entrez.efetch(
    db="nucleotide",
    rettype="fasta",
    retmode="text",
    retmax=len(record["IdList"]),
    webenv=webenv,
    query_key=query_key,
    idtype="acc",
)

dataNRPS = fetch.read()
with open("fungiRPBS_AD.fa", 'w') as f:
    f.write(dataNRPS)
    fetch.close()


## plants

justPlantsNRPShandle = Entrez.esearch(db="nucleotide", 
    term="(non-ribosomal peptide synthetase) AND green plants[porgn:__txid33090] AND biomol_mRNA[Properties]",
    idtype="acc",
    retmax=100,
    usehistory="y",
)

record = Entrez.read(justPlantsNRPShandle)
webenv = record["WebEnv"]
query_key = record["QueryKey"]
fetch = Entrez.efetch(
    db="nucleotide",
    rettype="fasta",
    retmode="text",
    retmax=len(record["IdList"]),
    webenv=webenv,
    query_key=query_key,
    idtype="acc",
)

dataNRPS = fetch.read()
with open("plantsRPBS_AD.fa", 'w') as f:
    f.write(dataNRPS)
    fetch.close()

## bacteria. Not sure why, can't get mRNA, so limit length of sequences
justBactNRPShandle = Entrez.esearch(db="nucleotide", 
    #term="(non-ribosomal peptide synthetase) AND bacteria [ORGN] AND biomol_mRNA[Properties]", ## doesn't work, 0 results
    term="(non-ribosomal peptide synthetase) AND bacteria [ORGN] AND (0[SLEN] : 20000[SLEN]))",
    idtype="acc",
    retmax=25,
    usehistory="y",
)

record = Entrez.read(justBactNRPShandle)
webenv = record["WebEnv"]
query_key = record["QueryKey"]

fetch = Entrez.efetch(
    db="nucleotide",
    rettype="fasta",
    retmode="text",
    retmax=len(record["IdList"]),
    webenv=webenv,
    query_key=query_key,
    idtype="acc",
)

dataNRPS = fetch.read()
with open("bactRPBS_AD.fa", 'w') as f:
    f.write(dataNRPS)
    fetch.close()

## the plan would be, try these alignments 
## then look for the existing primer sites
## check length and diversity, report

## the reduced dataset also failed, though it made it further, into the alignment 
## stage....

## let's try the fungal only file:

muscle -in fungiRPBS_AD.fa  -out Fungi_AD_aligned.afa #-maxiters 2
## that worked, with 20 sequences...
muscle -in plantsRPBS_AD.fa  -out plantsNRPS_aligned.afa #-maxiters 2

muscle -in bactRPBS_AD.fa  -out bactNRPs_aligned.afa #-maxiters 2

## not sure, my acronyms got mixed up there a bit...

GCNTACNNNATNTACACNTCNGG
CCNGANGTGTANATNNNGTANGC
NANGTCNCCNGTNCGGTA
TACCGNACNGGNGACNTN

A3F=GC.TAC...AT.TACAC.TC.GG
A3Frc=CC.GA.GTGTA.AT...GTA.GC
A7R=.A.GTC.CC.GT.CGGTA
A7Rrc=TACCG.AC.GG.GAC.T.

## check fungi with these
grep $A3F fungiRPBS_AD.fa
grep $A3Frc fungiRPBS_AD.fa
grep $A7R fungiRPBS_AD.fa
grep $A7Rrc fungiRPBS_AD.fa ## hit, 2

grep $A3F bactRPBS_AD.fa
grep $A3Frc bactRPBS_AD.fa ## hit
grep $A7R bactRPBS_AD.fa ## hit
grep $A7Rrc bactRPBS_AD.fa

grep $A3F plantsRPBS_AD.fa
grep $A3Frc plantsRPBS_AD.fa
grep $A7R plantsRPBS_AD.fa
grep $A7Rrc plantsRPBS_AD.fa


## the following was for quickly testing the number of sequences 
## number of seqs:
ns = 5
fa = 0
infiles = ["plantsRPBS_AD.fa", "bactRPBS_AD.fa", "fungiRPBS_AD.fa"]
shortfiles = ["plantsRPBS_AD_short.fa", "bactRPBS_AD_short.fa", "fungiRPBS_AD_short.fa"]
outfiles = ["plantsRPBS_AD.afa", "bactRPBS_AD.afa", "fungiRPBS_AD.afa"]
ADparse = SeqIO.parse(infiles[fa], "fasta")
AdparseL = list(ADparse)
## first we'll see if we just need to reduce the size of the alignment:
with open (shortfiles[fa], "w") as f:
    SeqIO.write(AdparseL[0:ns], f, "fasta")
    #musc = MuscleCommandline(input=shortfiles[fa], out=outfiles[fa])
    ## run the alignment from here? 
    #musc()

## not working to run in biopython...


## looking at this, these are entire large scaffolds, and mRNA all mixed up.
## we want to exclude scaffolds, chromosomes, etc...we need a gene based 
## approach...
########## blasting genbank #######

## fuck all that for the moment... 
## Instead, for the moment, we need to flip this on its head and blast 
## genbank with these primers, see what results. 

## how do we do this?


NCBIWWW.qblast()

## example from the documentation
result_handle = NCBIWWW.qblast("blastn", "nt", "8332116")

## to look for our primers:

## for the NRPS enzyme, they use 

A3F=Bio.Seq.Seq('GCSTACSYSATSTACACSTCSGG')
A7R=Bio.Seq.Seq('SASGTCVCCSGTSCGGTA')

checkNRPSfor = NCBIWWW.qblast("blastn", "nt", A3F)

## yields nothing. probably because of all the ambiguity 
## what if we replace these with N?

## a good page for the ambiguous abbreviations:
## https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&PAGE_TYPE=BlastDocs&DOC_TYPE=BlastHelp

A3F  =   Bio.Seq.Seq('GCSTACSYSATSTACACSTCSGG')

A3FNNN = Bio.Seq.Seq('GCNTACNNNATNTACACNTCNGG')

checkNRPSfor = NCBIWWW.qblast("blastn", "nt", A3FNNN)

## write this:

with open("A3Fcheck.txt", "w") as out_handle:
    out_handle.write(checkNRPSfor.read())

result_handle.close()

## not working...there are some tips on this page: 

## https://eu.idtdna.com/pages/education/decoded/article/tips-for-using-blast-to-locate-pcr-primers

A3FA7RN = Bio.Seq.Seq('GCNTACNNNATNTACACNTCNGGNNNNNNNNNNNNNNNNNNNNNNNANGTCNCCNGTNCGGTA')

checkNRPSfor = NCBIWWW.qblast("blastn", "nt", A3FA7RN)

## yeah that does nothing...hmmm...

## seems like blasting primer sets just isn't going to work.

## so skip that for now...

## but we have other jobs for genbank. We need a list of useful metabolites to look for, 
## so we can design primers. 
## who am I kidding, I can't design primers. 

## I want to see if our PKS and NRP

## for each, say something interesting, list available literature and 
## accessions/genomes,  find a primer set if available

## for pathogen resistence, we were interested in antimicrobials. 

## antifungals: 
##   echinocandin (NRP lipopeptide, fungal origin)
##   cyclosporing (cyclic NRP, fungal origin)
##   polyenes (polyketide, bacterial origin, maybe also in Fungi?)
##   strobilurins (polyketide, fungal origin, mostly basidios?)

## for drought, antioxidents and osmolytes

## drought & exposure
##   fungal antioxidants:
##    SOD, CAT
##   fungal osmolytes:
##    manitol?

## anti-herbivory
##   ergotamine
##   other alkaloids?

##  

## change of plan - use targeted PCR tests for endophytes, and shotgun metagenomics for 
## epiphytes and soil

## how many samples? Kyle did ten... doesn't seem like much but they still used 
## 21 lanes of hi-seq, jeezus, even at 8 lanes/run that thats 2.5 runs...

## I think we will have to be a bit courser, given our budget. 

## given our budget of $4000, I think we have to stick with metabarcoding + PCR
## but incorporate epiphytes into the samples. Root and leaf washing, pelletizing,
## DNA extraction. Targeted PCR of fungal and bacterial products of interest. 

## need to get draft today. Plan:

## 1 - set up massive searches of plant, fungal, bacterial for primers. Let them 
## run.

## 2 - write draft and budget for advisors

####################################

## 1 - set up searches for primers

## basic idea 
## 1) search and download LOTS of sequences from each category
## 2) set them up as generators
## 3) look for exact matches of our primers in each sequence, tally as you go


## 
## 1 Search and download sequences:

## try method with a few fungal, then expand:
justFungiNRPShandle = Entrez.esearch(db="nucleotide", 
    term="(non-ribosomal peptide synthetase) AND Fungi[Organism] AND biomol_mRNA[Properties]",
    idtype="acc",
    retmax=1000,
    usehistory="y",
)

record = Entrez.read(justFungiNRPShandle)
webenv = record["WebEnv"]
query_key = record["QueryKey"]
# len(record["IdList"])

fetch = Entrez.efetch(
    db="nucleotide",
    rettype="fasta",
    retmode="text",
    retmax=len(record["IdList"]),
    webenv=webenv,
    query_key=query_key,
    idtype="acc",
)

dataNRPS = fetch.read()
with open("fungalNRPS.fa", 'w') as f:
    f.write(dataNRPS)
    fetch.close()

## identify our primers:
A3F = Bio.Seq.Seq("CNTACNNNATNTACACNTCNGG")
A7R = Bio.Seq.Seq("ANGTCNCCNGTNCGGTA")
A3Frc = Bio.Seq.Seq("CCNGANGTGTANATNNNGTANG")
A7Rrc = Bio.Seq.Seq("TACCGNACNGGNGACNT")
## or for regex
A3Frg="C.TAC...AT.TACAC.TC.GG"
A7Rrg="A.GTC.CC.GT.CGGTA"
A3Frgrc="CC.GA.GTGTA.AT...GTA.G"
A7Rrgrc="TACCG.AC.GG.GAC.T"

## can we open this directly as a set of seqs?

fungalRecords=list()
matchsNRPS_forwardPrime = list()
matchsNRPS_forwardPrime_RC = list()
matchsNRPS_reversePrime = list()
matchsNRPS_reversePrime_RC = list()
for i in SeqIO.parse("fungalNRPS.fa", "fasta"): 
    ## forward A3F
    forwardPrime = re.search(A3Frg, str(i.seq))
    if forwardPrime: 
        matchsNRPS_forwardPrime.append(forwardPrime.group(0))
        fungalRecords.append(i.description)
    ## forward A3F, reverse comp
    forwardPrime_RC = re.search(A3Frgrc, str(i.seq))
    if forwardPrime_RC: 
        matchsNRPS_forwardPrime_RC.append(forwardPrime.group(0))
        fungalRecords.append(i.description)
    ## reverse A7R primer
    reversePrim = re.search(A7Rrg, str(i.seq))
    if reversePrim: 
        matchsNRPS_reversePrime.append(reversePrim_RC.group(0))
        fungalRecords.append(i.description)
    ## reverse A7R primer, reverse comp 
    reversePrim_RC = re.search(A7Rrgrc, str(i.seq))
    if reversePrim_RC: 
        matchsNRPS_reversePrime_RC.append(reversePrim_RC.group(0))
        fungalRecords.append(i.description)

len(fungalRecords) ## 42
len(set(fungalRecords)) ## 40, so only 2 with both primers?
## =two repeats...so not a lot of complete primer pairs to work with. 
## 42 out of 294 records ~14%, Compare this to bacteria...

len(record["IdList"])

matchsNRPS_forwardPrime
matchsNRPS_forwardPrime_RC
matchsNRPS_reversePrime
matchsNRPS_reversePrime_RC

## so these show some activity in fungi. How extensive, needs more time than I have. 

## do this for bacteria:

justBactNRPShandle = Entrez.esearch(db="nucleotide", 
    #term="(non-ribosomal peptide synthetase) AND bacteria [ORGN] AND biomol_mRNA[Properties]", ## doesn't work, 0 results
    term="(non-ribosomal peptide synthetase) AND bacteria [ORGN] AND (0[SLEN] : 20000[SLEN]))",
    idtype="acc",
    retmax=1000,
    usehistory="y",
)

record = Entrez.read(justBactNRPShandle)
webenv = record["WebEnv"]
query_key = record["QueryKey"]

fetch = Entrez.efetch(
    db="nucleotide",
    rettype="fasta",
    retmode="text",
    retmax=len(record["IdList"]),
    webenv=webenv,
    query_key=query_key,
    idtype="acc",
)

dataNRPS = fetch.read()
with open("bactNRPS.fa", 'w') as f:
    f.write(dataNRPS)
    fetch.close()

## now look through these:
bacterialRecords=list()
matchsNRPS_forwardPrime = list()
matchsNRPS_forwardPrime_RC = list()
matchsNRPS_reversePrime = list()
matchsNRPS_reversePrime_RC = list()
## BiodiversityGenomics
for i in SeqIO.parse("bactNRPS.fa", "fasta"): 
    ## forward A3F
    forwardPrime = re.search(A3Frg, str(i.seq))
    if forwardPrime: 
        matchsNRPS_forwardPrime.append(forwardPrime.group(0))
        bacterialRecords.append(i.description)
    ## forward A3F, reverse comp
    forwardPrime_RC = re.search(A3Frgrc, str(i.seq))
    if forwardPrime_RC: 
        matchsNRPS_forwardPrime_RC.append(forwardPrime_RC.group(0))
        bacterialRecords.append(i.description)
    ## reverse A7R primer
    reversePrim = re.search(A7Rrg, str(i.seq))
    if reversePrim: 
        matchsNRPS_reversePrime.append(reversePrim.group(0))
        bacterialRecords.append(i.description)
    ## reverse A7R primer, reverse comp 
    reversePrim_RC = re.search(A7Rrgrc, str(i.seq))
    if reversePrim_RC: 
        matchsNRPS_reversePrime_RC.append(reversePrim_RC.group(0))
        bacterialRecords.append(i.description)


## definitely in there

len(bacterialRecords) ## = 242
len(set(bacterialRecords)) ## 203, 20% of total records. 

matchsNRPS_forwardPrime
matchsNRPS_forwardPrime_RC
matchsNRPS_reversePrime
matchsNRPS_reversePrime_RC

## check plants:

justPlantsNRPShandle = Entrez.esearch(db="nucleotide", 
    term="(non-ribosomal peptide synthetase) AND green plants[porgn:__txid33090] AND biomol_mRNA[Properties]",
    idtype="acc",
    retmax=1000,
    usehistory="y",
)

record = Entrez.read(justPlantsNRPShandle)
webenv = record["WebEnv"]
query_key = record["QueryKey"]
fetch = Entrez.efetch(
    db="nucleotide",
    rettype="fasta",
    retmode="text",
    retmax=len(record["IdList"]),
    webenv=webenv,
    query_key=query_key,
    idtype="acc",
)

dataNRPS = fetch.read()
with open("plantNRPS.fa", 'w') as f:
    f.write(dataNRPS)
    fetch.close()


plantRecords=list()
matchsNRPS_forwardPrime = list()
matchsNRPS_forwardPrime_RC = list()
matchsNRPS_reversePrime = list()
matchsNRPS_reversePrime_RC = list()
for i in SeqIO.parse("plantNRPS.fa", "fasta"): 
    ## forward A3F
    forwardPrime = re.search(A3Frg, str(i.seq))
    if forwardPrime: 
        matchsNRPS_forwardPrime.append(forwardPrime.group(0))
        plantRecords.append(i.description)
    ## forward A3F, reverse comp
    forwardPrime_RC = re.search(A3Frgrc, str(i.seq))
    if forwardPrime_RC: 
        matchsNRPS_forwardPrime_RC.append(forwardPrime_RC.group(0))
        plantRecords.append(i.description)
    ## reverse A7R primer
    reversePrim = re.search(A7Rrg, str(i.seq))
    if reversePrim: 
        matchsNRPS_reversePrime.append(reversePrim_RC.group(0))
        plantRecords.append(i.description)
    ## reverse A7R primer, reverse comp 
    reversePrim_RC = re.search(A7Rrgrc, str(i.seq))
    if reversePrim_RC: 
        matchsNRPS_reversePrime_RC.append(reversePrim_RC.group(0))
        plantRecords.append(i.description)

len(plantRecords) ## 

len(record["IdList"])

matchsNRPS_forwardPrime
matchsNRPS_forwardPrime_RC
matchsNRPS_reversePrime
matchsNRPS_reversePrime_RC

## not picking up anything...
## grep?

grep $A3Frg plantNRPS.fa | wc -l
grep $A7Rrg plantNRPS.fa | wc -l
grep $A3Frgrc plantNRPS.fa | wc -l
grep $A7Rrgrc plantNRPS.fa | wc -l

## no evidence of plant use of these primers. 
## then again, there were only 15 sequences...
## they are probably mis-annotated by automated pipelines.

## okay, what else do we need?

## can we repeat the above, with PKS? 

############################# PKS ############################# 
## now to repeat with PKS.

## I get the impression this is a much larger family of enzymes
## with more activity in plants...

## I also wonder if there is a divide here between bacteria 
## and fungi. the lit says that one pattern is that bacteria
## use multimodular, non-interative PKS-Is, and fungi tend to 
## use a monomodular iterative PKS. But both have KS domains, 
## which are presumably pretty conserved and these primers 
## are pretty darn degenerate...


 the following search on genbank:

(Polyketide Synthase[Protein Name] OR (Polyketide[All Fields] AND Synthase[All Fields])) 
AND iterative[All Fields] AND (type I[Protein Name] OR type[All Fields])


## in the protein database gives us 2,776 records for fungi,
## and only 165 for bacteria. 

## a better search might be:
(((Polyketide Synthase) AND Type I) NOT Type II) NOT Type III 

## how can we get nucleotide info for these, if it is avaiable?

## bacteria
bactPKShandle = Entrez.esearch(db="nucleotide", 
    term="(((Polyketide Synthase) AND Type I) NOT Type II) NOT Type III AND bacteria [ORGN]", 
    idtype="acc",
    retmax=1000,
    usehistory="y",
)
 
record = Entrez.read(bactPKShandle)
webenv = record["WebEnv"]
query_key = record["QueryKey"]
fetch = Entrez.efetch(
    db="nucleotide",
    rettype="fasta",
    retmode="text",
    retmax=len(record["IdList"]),
    webenv=webenv,
    query_key=query_key,
    idtype="acc",
)

dataNRPS = fetch.read()

pksfile='/home/daniel/Documents/job_apps/panama/bigFilesBiodiv/bactPKS.fa'
with open(pksfile, 'w') as f:
    f.write(dataNRPS)
    fetch.close()

## our polyketide primers look like this:

## KS [degKS2F.i (5′-GCIATGGAYCCICARCARMGIVT)
## degKS2R.i (5′-GTICCIGTICCRTGISCYTCIAC)

## so for general purpose biopython manipulation
## sub out the non-GCAT bases:

KS2Fraw = "GCIATGGAYCCICARCARMGIVT"
KS2F = re.sub('[^GCAT]',".", KS2Fraw)
KS2Fseq = Bio.Seq.Seq(KS2F) 
KS2Fseq_RC = KS2Fseq.reverse_complement()
KS2F_RC = str(KS2Fseq_RC)
KS2Rraw = 'GTICCIGTICCRTGISCYTCIAC'
KS2R = re.sub('[^GCAT]',".", KS2Rraw)
KS2Rseq = Bio.Seq.Seq(KS2R) 
KS2Rseq_RC = KS2Rseq.reverse_complement()
KS2R_RC = str(KS2Rseq_RC)

## can we now peruse our bacterial PKS records with this?

PKS_for = list()
PKS_for_RC = list()
PKS_rev = list()
PKS_rev_RC = list()
for i in SeqIO.parse(pksfile, "fasta"): 
    ## forward pks
    match_i = re.search(KS2F, str(i.seq))
    if match_i: 
        bs=Bio.SeqRecord.SeqRecord(
                                Bio.Seq.Seq(match_i.group()),
                                id=i.id,
                                description=i.description,
                                )
        PKS_for.append(bs)
    ## forward pks, reverse comp
    match_i = re.search(KS2F_RC, str(i.seq))
    if match_i: 
        bs=Bio.SeqRecord.SeqRecord(
                                Bio.Seq.Seq(match_i.group()),
                                id=i.id,
                                description=i.description,
                                )
        PKS_for_RC.append(bs)
    ## reverse pks primer
    match_i = re.search(KS2R, str(i.seq))
    if match_i: 
        bs=Bio.SeqRecord.SeqRecord(
                                Bio.Seq.Seq(match_i.group()),
                                id=i.id,
                                description=i.description,
                                )
        PKS_rev.append(bs)
    # reverse pks primer, reverse comp 
    match_i = re.search(KS2R_RC, str(i.seq))
    if match_i: 
        bs=Bio.SeqRecord.SeqRecord(
                                Bio.Seq.Seq(match_i.group()),
                                id=i.id,
                                description=i.description,
                                )
        PKS_rev_RC.append(bs)


## now write these out as fastas:
org = 'bact'
SeqIO.write(PKS_for, str("PKS_for_"+org+".fa"), "fasta")
SeqIO.write(PKS_for_RC, str("PKS_for_RC_"+org+".fa"), "fasta")
SeqIO.write(PKS_rev, str("PKS_rev_"+org+".fa"), "fasta")
SeqIO.write(PKS_rev_RC, str("PKS_rev_RC_"+org+".fa"), "fasta")

## not finding. 

### grep checks

KS2F=GC.ATGGA.CC.CA.CA..G..T
KS2F_RC=A..C..TG.TG.GG.TCCAT.GC
KS2R=GT.CC.GT.CC.TG..C.TC.AC
KS2R_RC=GT.GA.G..CA.GG.AC.GG.AC
#pksfile='/home/daniel/Documents/job_apps/panama/bigFilesBiodiv/bactPKS.fa'
#pksfile='/home/daniel/Documents/job_apps/panama/bigFilesBiodiv/plantPKS.fa'

grep $KS2F $pksfile | wc -l 
grep $KS2F_RC $pksfile | wc -l 
grep $KS2R $pksfile | wc -l 
grep $KS2R_RC $pksfile | wc -l 

grep $KS2R_RC $pksfile 

## definitely in there
## go back fix the python code

######################### fungi pks #########################

## fungi for pks
pksfile='/home/daniel/Documents/job_apps/panama/bigFilesBiodiv/fungiPKS.fa'
fungiPKShandle = Entrez.esearch(db="nucleotide", 
    term="(((Polyketide Synthase) AND Type I) NOT Type II) NOT Type III AND fungi [ORGN]", 
    idtype="acc",
    retmax=1000,
    usehistory="y",
)

record = Entrez.read(fungiPKShandle)
webenv = record["WebEnv"]
query_key = record["QueryKey"]
fetch = Entrez.efetch(
    db="nucleotide",
    rettype="fasta",
    retmode="text",
    retmax=len(record["IdList"]),
    webenv=webenv,
    query_key=query_key,
    idtype="acc",
)

dataNRPS = fetch.read()

with open(pksfile, 'w') as f:
    f.write(dataNRPS)
    fetch.close()

PKS_for = list()
PKS_for_RC = list()
PKS_rev = list()
PKS_rev_RC = list()
for i in SeqIO.parse(pksfile, "fasta"): 
    ## forward pks
    match_i = re.search(KS2F, str(i.seq))
    if match_i: 
        bs=Bio.SeqRecord.SeqRecord(
                                Bio.Seq.Seq(match_i.group()),
                                id=i.id,
                                description=i.description,
                                )
        PKS_for.append(bs)
    ## forward pks, reverse comp
    match_i = re.search(KS2F_RC, str(i.seq))
    if match_i: 
        bs=Bio.SeqRecord.SeqRecord(
                                Bio.Seq.Seq(match_i.group()),
                                id=i.id,
                                description=i.description,
                                )
        PKS_for_RC.append(bs)
    ## reverse pks primer
    match_i = re.search(KS2R, str(i.seq))
    if match_i: 
        bs=Bio.SeqRecord.SeqRecord(
                                Bio.Seq.Seq(match_i.group()),
                                id=i.id,
                                description=i.description,
                                )
        PKS_rev.append(bs)
    # reverse pks primer, reverse comp 
    match_i = re.search(KS2R_RC, str(i.seq))
    if match_i: 
        bs=Bio.SeqRecord.SeqRecord(
                                Bio.Seq.Seq(match_i.group()),
                                id=i.id,
                                description=i.description,
                                )
        PKS_rev_RC.append(bs)


## now write these out as fastas:
org = 'fungi'
SeqIO.write(PKS_for, str("PKS_for_"+org+".fa"), "fasta")
SeqIO.write(PKS_for_RC, str("PKS_for_RC_"+org+".fa"), "fasta")
SeqIO.write(PKS_rev, str("PKS_rev_"+org+".fa"), "fasta")
SeqIO.write(PKS_rev_RC, str("PKS_rev_RC_"+org+".fa"), "fasta")

######################### plant pks #########################

## plant for pks

plantPKShandle = Entrez.esearch(db="nucleotide", 
    term="(((Polyketide Synthase) AND Type I) NOT Type II) NOT Type III AND green plants[porgn:__txid33090]", 
    idtype="acc",
    retmax=1000,
    usehistory="y",
)
record = Entrez.read(plantPKShandle)
record['Count']
webenv = record["WebEnv"]
query_key = record["QueryKey"]

fetch = Entrez.efetch(
    db="nucleotide",
    rettype="fasta",
    retmode="text",
    retmax=len(record["IdList"]),
    webenv=webenv,
    query_key=query_key,
    idtype="acc",
)

dataNRPS = fetch.read()

## how many 

pksfile='/home/daniel/Documents/job_apps/panama/bigFilesBiodiv/plantPKS.fa'
with open(pksfile, 'w') as f:
    f.write(dataNRPS)
    fetch.close()

len(record["IdList"])

PKS_for = list()
PKS_for_RC = list()
PKS_rev = list()
PKS_rev_RC = list()
for i in SeqIO.parse(pksfile, "fasta"): 
    ## forward pks
    match_i = re.search(KS2F, str(i.seq))
    if match_i: 
        bs=Bio.SeqRecord.SeqRecord(
                                Bio.Seq.Seq(match_i.group()),
                                id=i.id,
                                description=i.description,
                                )
        PKS_for.append(bs)
    ## forward pks, reverse comp
    match_i = re.search(KS2F_RC, str(i.seq))
    if match_i: 
        bs=Bio.SeqRecord.SeqRecord(
                                Bio.Seq.Seq(match_i.group()),
                                id=i.id,
                                description=i.description,
                                )
        PKS_for_RC.append(bs)
    ## reverse pks primer
    match_i = re.search(KS2R, str(i.seq))
    if match_i: 
        bs=Bio.SeqRecord.SeqRecord(
                                Bio.Seq.Seq(match_i.group()),
                                id=i.id,
                                description=i.description,
                                )
        PKS_rev.append(bs)
    # reverse pks primer, reverse comp 
    match_i = re.search(KS2R_RC, str(i.seq))
    if match_i: 
        bs=Bio.SeqRecord.SeqRecord(
                                Bio.Seq.Seq(match_i.group()),
                                id=i.id,
                                description=i.description,
                                )
        PKS_rev_RC.append(bs)


## now write these out as fastas:

org = 'plant'
SeqIO.write(PKS_for, str("PKS_for_"+org+".fa"), "fasta")
SeqIO.write(PKS_for_RC, str("PKS_for_RC_"+org+".fa"), "fasta")
SeqIO.write(PKS_rev, str("PKS_rev_"+org+".fa"), "fasta")
SeqIO.write(PKS_rev_RC, str("PKS_rev_RC_"+org+".fa"), "fasta")

## how hard is it to to get an aligment of out these?

muscle -in PKS_for_plant.fa  -out PKS_for_plant.afa

muscle -in PKS_for_bact.fa  -out PKS_for_bact.afa

## that was easy, even with ~350 seqs..

## does muscle require correction of reverse compliments?
muscle -in <(cat PKS_for_bact.fa PKS_for_bact_RC.fa) -out testCombo.afa

## looks good.

## can alv show these file formats?
alv testCombo.afa

## neat. will that work in a jupyter notebook?

## not really. let's make some nice graphs in ugene and put them in there
## for now. 

## in the future, I think we need something like:
https://dmnfarrell.github.io/bioinformatics/bokeh-sequence-aligner
 
## anyway...what do we want to show?

## maybe four alignments, NRPS forward and back, PKS forward and back, all organisms

## NRPS is a mess. 

## not sure how to summarize 

## redo bacterial PKS sequences sources, they are almost entirely mycobacterium.

#### ChenDu diagram ####

## let's see if we can get the sequence information for 
## the sequences used to construct the Chen and Du 2015 
## tree for iterative polyketide synthases

## we especially want the AT domains from these. 

## we also want to check non iterative PKSIs 
## too much. But let's see what we can do today.


## to start, can we get their accessions?

## ugh, how do we do this again? use entrez package. 

## let's first just try to get their KS domain sequences

## list of their accession numbers:

aa = pd.read_csv('ChenDuAccessions.csv', header=None)
aa.columns = ['name', 'id','NorP']
accessions=aa.id.to_list()

## we had to update a few accessions numbers
## and in one the sequence data wasn't available: 

## KF954512.1 / WP_011873136.1 "suppressed" by genbank, no nuccore entry, just prot
## <https://www.ncbi.nlm.nih.gov/protein/WP_011873136.1/>

## also, ZP_11383500.1 was not found, changed for nucleotide id KU597647.1, think this is the correct entry


## example from biopython
handle = Entrez.efetch(db="nucleotide", id="AY851612", rettype="gb", retmode="text")
aa = handle.read()

pprint.pprint(aa)

## our data like this? from above:

fetch = Entrez.efetch(
    db="protein",
    rettype="fasta",
    retmode="text",
    id=accessions,
)

## that works, to get the translated protein. But we want the gene sequence...

#look4nuc = Entrez.elink(dbfrom='protein', db='nuccore', id=accessions)

look4nuc = Entrez.elink(dbfrom='protein', db='nucleotide', id="AAD43562.2")

aa = Entrez.read(look4nuc)

## in the case of a single accession, and single result:
idd = aa[0]['LinkSetDb'][0]['Link'][0]['Id']

bb = Entrez.efetch(
    db="nuccore",
    rettype="fasta",
    retmode="text",
    id=idd
)

## can we make a biopython record directly for this?

## Bio.SeqIO has what we need:

record = SeqIO.read( bb, "fasta" )


## okay, so to loop through our accessions and get the fastas we need:

chenDu = pd.read_csv('ChenDuAccessions.csv', header=None)
chenDu.columns = ['name', 'id','NorP','bactOrFungi','nucID']
## our proteins are here:
prots = chenDu[chenDu['NorP'] == 'p'].id.to_list()
## remove problematic protein sequence:
prots.remove('WP_011873136.1')

## get ChenDu nucleotide sequences that are listed by prot accessions:
for i in prots:
    print(i)
    look4nuc = Entrez.elink(dbfrom='protein', db='nucleotide', id=i)
    aa = Entrez.read(look4nuc)
    idd = aa[0]['LinkSetDb'][0]['Link'][0]['Id']
    bb = Entrez.efetch(
        db="nuccore",
        rettype="fasta",
        retmode="text",
        id=idd
    )
    record = SeqIO.read( bb, "fasta" )
    print(i+" is "+ record.id)
    SeqIO.write(record, handle=record.id+'.fa', format='fasta')

## add those nuseq ids to the ChenDu panda

## get the ChenDu accessions that are already nutide seqs:

aa = pd.read_csv('ChenDuAccessions.csv', header=None)
aa.columns = ['name', 'id','NorP']
nucs = aa[aa['NorP'] == 'n'].id.to_list()
for i in nucs:
    print(i)
    bb = Entrez.efetch(
        db="nuccore",
        rettype="fasta",
        retmode="text",
        id=i
    )
    record = SeqIO.read( bb, "fasta" )
    SeqIO.write(record, handle=record.id+'.fa', format='fasta')


## for deleting first mistakes:
#find . -maxdepth 1 -newermt "2020-12-13" -name "*.fa" -exec rm {} \;

#find . -maxdepth 1 -newermt "2020-12-15" -name "*.fa" -exec rm {} \;

## okay, we should have (almost) all the sequences that ChenDu used for
## for their tree of KS homology...can we find our primers in these 
## files?

os.makedirs('./ChenDuAccs')
#find . -maxdepth 1 -newermt "2020-12-13" -name "*.fa" -exec mv '{}' ChenDuAccs/ \;

## how do we search these for the KS regions, where our primers are from?
## or should we just search for our primers...

## primer sequence for KS:

KS2F GCIATGGAYCCICARCARMGIVT
KS2R GTICCIGTICCRTGISCYTCIAC

## this is a good page for this: 
## <https://en.wikipedia.org/wiki/Nucleotide#Abbreviation_codes_for_degenerate_bases>
## 'I' is for inosine, pairs with A, C, or T
## so, I = [TGA]
## not sure, try it.

KS2F = Bio.Seq.Seq("GCIATGGAYCCICARCARMGIVT")

KS2R = Bio.Seq.Seq("GTICCIGTICCRTGISCYTCIAC")

## if we translate this into regex, I think it would look like this:

############################################
##    GC  I  ATGGA Y  CC  I  CA R  CA R   M  G  I   V   T
KS2F="GC[TGA]ATGGA[CT]CC[TGA]CA[AG]CA[AG][AC]G[TGA][ACG]T"
############################################


############################################
## reverse primer
##      GT  I  CC  I  GT  I  CC R  TG  I   S  C Y  TC  I  AC
KS2R = "GT[TGA]CC[TGA]GT[TGA]CC[AG]TG[TGA][CG]C[CT]TC[TGA]AC"
############################################

## to search a sequence:


p = re.compile(KS2F)
aa = "ACAGAGAATGCTCCCCCGCTATGGACCCACAACAGAGAATGCTCCCCCCCCCCCCCCCAGAGAATGCTATGGACCCACAACAGAGAATGCTCCCCCCCCCCCCCCCAGAGAAT"
bb = p.search(aa)
if bb is not None: 
    b, e = p.search(aa).span()
    print('KS2F')
    print(KS2F)
    print(aa[b:e])
else: print ('not found')

p = re.compile(KS2R)
aa = 'GTAGGGGGGGGGGGGGCTTCTACACCTTCTACGTACCGGTGCCGTGGTACCGGTGCCGTGACCTTCTACACCTTCTACGTACCGGTGCCGTGTACCGGTGCCGTGGTACCGGTG'
if bb is not None: 
    b, e = p.search(aa).span()
    print('KS2R')
    print(KS2R)
    print(aa[b:e])
else: print ('not found')

## seems to work. 
## how about reverse complements?

KS2Fseq = Bio.Seq.Seq(KS2F)

str(KS2Fseq.reverse_complement())
## almost works, but brackets...fix manually?

############################################
##       'A]CGT[]TCA[C]GT[]CT[TG]CT[TG]TCA[GG]AG[TCCAT]TCA[GC'
KS2Frc = 'A[CGT][TCA]C[GT][CT]TG[CT]TG[TCA]GG[AG]TCCAT[TCA]GC'
############################################

## check them out:
KS2F
KS2Frc
## looks okay, try a sample search:

'TCCATCGCTGTGGATCTGTGGATCTGTAGTCGCTGTTGTGGATCCATCGCTGTGGATCTGTGGATCTGTGGATCTGTA'

p = re.compile(KS2Frc)
aa = 'TCCATCGCTGTGGATCTGTGGATCTGTAGTCGCTGTTGTGGATCCATCGCTGTGGATCTGTGGATCTGTGGATCTGTA'
bb = p.search(aa)
if bb is not None: 
    b, e = p.search(aa).span()
    print('KS2Frc')
    print(KS2Frc)
    print(aa[b:e])
else: print ('not found')

## okay, get rc of reverse primer

KS2Rseq = Bio.Seq.Seq(KS2R)

str(KS2Rseq.reverse_complement())
## almost works, but brackets...fix manually?

############################################
##       'GT]TCA[GA]AG[G]CG[]TCA[CA]CT[GG]TCA[AC]TCA[GG]TCA[AC'
KS2Rrc = 'GT[TCA]GA[AG]G[CG][TCA]CA[CT]GG[TCA]AC[TCA]GG[TCA]AC'
############################################

## check them out:
KS2R
KS2Rrc
## looks okay, try a sample search:

p = re.compile(KS2Rrc)
aa = 'GAACGGAGTCGAGGCTCACGGTCGAAGGTCATGGCACTGGAACGGCTCACGGAACTGGCACGGAACGGAACGGAACGTCGAAGGTCATGGCACTGGAAC'
#aa = 'GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG'
bb = p.search(aa)
cc = p.findall(aa)
dd = list(p.finditer(aa))
ee = [ i.span() for i in dd ]

if bb is not None: 
    b, e = p.search(aa).span()
    print('KS2Rrc')
    print(KS2Rrc)
    print(aa[b:e])
else: print ('not found')

## looks okay. 

## to sum up:

### primers and primer RC regexes ###
KS2F="GC[TGA]ATGGA[CT]CC[TGA]CA[AG]CA[AG][AC]G[TGA][ACG]T"
KS2R = "GT[TGA]CC[TGA]GT[TGA]CC[AG]TG[TGA][CG]C[CT]TC[TGA]AC"
KS2Frc = 'A[CGT][TCA]C[GT][CT]TG[CT]TG[TCA]GG[AG]TCCAT[TCA]GC'
KS2Rrc = 'GT[TCA]GA[AG]G[CG][TCA]CA[CT]GG[TCA]AC[TCA]GG[TCA]AC'
### ----------------------------- ###

########################

## can we search our various fastas for these primers? 

KS2Fregex = re.compile(KS2F)
KS2Rregex = re.compile(KS2R)
KS2Frcregex = re.compile(KS2Frc)
KS2Rrcregex = re.compile(KS2Rrc)

hits = []
desc = []
for i in os.listdir('ChenDuAccs'):
    print(i)
    aa = SeqIO.parse(open(('ChenDuAccs/'+i), mode='r'), 'fasta')
    rec = list(aa)[0]
    if KS2Fregex.search(str(rec.seq)): hits.append(i); desc.append(rec.description)
    if KS2Rregex.search(str(rec.seq)): hits.append(i); desc.append(rec.description)
    if KS2Frcregex.search(str(rec.seq)): hits.append(i); desc.append(rec.description)
    if KS2Rrcregex.search(str(rec.seq)): hits.append(i); desc.append(rec.description)
    hits = [ i.replace('.fa', '') for i in hits ]
    hitsU = list(set(hits))


## only 5 non-redundant hits

## any fungal?

hasPrim = chenDu[chenDu['nucID'].isin(hitsU)]

hasPrim.reset_index(inplace=True, drop=True)

hasPrim

## yup, fumonisin gene cluster

## well, maybe we can take align these and take a look at them. 
## how do we get the regions of interest?

## according to the pnas article, we should 
## be looking at amplified regions of 350-500 bp
## does our one sequence with both primers bear this out?

## MonAI 
## AF440781.1

## how do we find the regions we need?


def getPrimerSpan(fafile, primer):
    """use one of the compiled regexes above"""
    with open((fafile), mode='r') as f:
        aa = SeqIO.parse(f, 'fasta')
        sequ = str(list(aa)[0].seq)
    primiter = primer.finditer(sequ)
    if primiter: 
        primerSpans = [ i.span() for i in list(primiter) ]
    else: 
        primerSpans = None
        print('primer not found')
    return(primerSpans)

fafile='ChenDuAccs/AF440781.1.fa'

## forward


getPrimerSpan(fafile=fafile, primer=KS2Fregex)

getPrimerSpan(fafile=fafile, primer=KS2Rregex)

getPrimerSpan(fafile=fafile, primer=KS2Frcregex)

getPrimerSpan(fafile=fafile, primer=KS2Rrcregex)

## these are all very far apart. 


## so what do we need to do? Need to be efficient about time here...

## These are pretty large proteins, average maybe 6,000 AAs. 
## so not going to try to repeat ChenDu's alignment. That is something
## I should get paid to do. 

## and we have to get on to the GIS stuff. can we extract the candidate regions 
## from the primer sites, knowing that they should be maybe 500 bp?

## pipeline for this:

## our fastas with the found primers are:

hasPrimerFiles = [ ("ChenDuAccs/"+i+".fa") for i in hasPrim.nucID.values ]


## in each of these, 
## if they have a forward primer or forward reverse comp, get the next 500 bp
## if they have a reverse or reverse comp, get the previous 500 bp

## try one
KS2Fregex 
KS2Rregex 
KS2Frcregex 
KS2Rrcregex 

hasPrimerFiles[0]

getPrimerSpan(hasPrimerFiles[0],KS2Fregex) ## two hits.
getPrimerSpan(hasPrimerFiles[0],KS2Frcregex) ## no
getPrimerSpan(hasPrimerFiles[0],KS2Rregex) ## three hits.
getPrimerSpan(hasPrimerFiles[0],KS2Rrcregex) ## none. 
## Odd.something may be wrong with our reverse complements

## to get the ~500 bp after each forward hit:

def getBPfromSite(fastafile, site, bp):
    """function to get sequence chunks from before or past a primer site."""
    with open(fastafile, mode='r') as f:
        seqIter = SeqIO.parse(f, 'fasta')
        sequ = list(seqIter)[0].seq
    if bp < 0:
        forfivehundo = sequ[(site[1]+bp):site[1]]
    elif bp > 0:
        forfivehundo = sequ[site[0]:(site[1]+bp)]
    elif bp == 0: 
        forfivehundo = None
    return(forfivehundo)

## okay, this takes one span and gets a sequence from one file

## how do we use this to get our sequences?

## gotta go through all 6 pks sequences, 
##    in each pks, check for presence of each primer
##        in each primer hit, get 500 bp sequence
 
## test with forward primer. so +500 bp

seqName=(hasPrim.nucID.values[i] + '_primer-' +'KS2F')
fastafile=hasPrimerFiles[i]
primer=KS2Fregex

def getSeqsForAPrimer(fastafile, primer, seqName):
    sites = getPrimerSpan(fastafile,primer)
    seqs=[]
    for sitenumber, site in enumerate(sites):
        sequence = getBPfromSite(hasPrimerFiles[0], site=site, bp=500)
        seqid = seqName+'_S'+str(sitenumber)
        seqRec = SeqRecord(sequence) 
        seqRec.id = seqid
        seqs.append(seqRec)
    return(seqs)

getPrimerSpan(fastafile,primer)

primerRegexes=[KS2Fregex, KS2Rregex, KS2Frcregex, KS2Rrcregex]

for i,j in enumerate(
seqName=(hasPrim.nucID.values[i] + '_primer-' +'KS2F')
fastafile=hasPrimerFiles[i]
primer=KS2Fregex
aa = getSeqsForAPrimer(fastafile, primer, seqName)

## write to a fasta file:
SeqIO.write(aa, 'testa.fasta', 'fasta')

## great. so do that for all primers, on all 6 sequences.
## probably reduce the length, to 100 bp?
## otherwise, alignment might be too much.

## back up, prep for jupyter in case this actually works. 
## here are the packages we need to bring in:

import pandas as pd
import shapely as sh
import numpy as np
import os, re, copy
from Bio import Entrez
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
Entrez.email = "danchurchthomas@gmail.com"

## and here are the functions we need:

def getPrimerSpan(fafile, primer):
    """use one of the compiled regexes above"""
    with open((fafile), mode='r') as f:
        aa = SeqIO.parse(f, 'fasta')
        sequ = str(list(aa)[0].seq)
    primiter = primer.finditer(sequ)
    if primiter: 
        primerSpans = [ i.span() for i in list(primiter) ]
    else: 
        primerSpans = None
        print('primer not found')
    return(primerSpans)

def getBPfromSite(fastafile, site, bp):
    """function to get sequence chunks from before or past a primer site."""
    with open(fastafile, mode='r') as f:
        seqIter = SeqIO.parse(f, 'fasta')
        sequ = list(seqIter)[0].seq
    if bp < 0:
        seqFrag = sequ[(site[1]+bp):site[1]]
    elif bp > 0:
        seqFrag = sequ[site[0]:(site[1]+bp)]
    elif bp == 0: 
        seqFrag = None
    return(seqFrag)

def getSeqsForAPrimer(fastafile, primer, seqName, seqLength):
    """use getPrimerSpan() and getBPfromSite() to collect the sequence
        fragments from each matching site to a primer and put into a 
        list of biopython sequence record objects"""
    sites = getPrimerSpan(fastafile,primer)
    seqs=[]
    for sitenumber, site in enumerate(sites):
        sequence = getBPfromSite(hasPrimerFiles[0], site=site, bp=seqLength)
        seqid = seqName+'_S'+str(sitenumber)
        seqRec = SeqRecord(sequence) 
        seqRec.id = seqid
        seqs.append(seqRec)
    return(seqs)

## primers and primer RC regexes ##
KS2F="GC[TGA]ATGGA[CT]CC[TGA]CA[AG]CA[AG][AC]G[TGA][ACG]T"
KS2R = "GT[TGA]CC[TGA]GT[TGA]CC[AG]TG[TGA][CG]C[CT]TC[TGA]AC"
KS2Frc = 'A[CGT][TCA]C[GT][CT]TG[CT]TG[TCA]GG[AG]TCCAT[TCA]GC'
KS2Rrc = 'GT[TCA]GA[AG]G[CG][TCA]CA[CT]GG[TCA]AC[TCA]GG[TCA]AC'
## compiled:
KS2Fregex = re.compile(KS2F)
KS2Rregex = re.compile(KS2R)
KS2Frcregex = re.compile(KS2Frc)
KS2Rrcregex = re.compile(KS2Rrc)

## make a series to iterate over
primerRegexes = [KS2Fregex, KS2Rregex, KS2Frcregex, KS2Rrcregex]
primerRegexesNames = ['KS2F', 'KS2R', 'KS2Frc', 'KS2Rrc']
primSeries = pd.Series(primerRegexes, index=primerRegexesNames)

## data
chenDu = pd.read_csv('ChenDuAccessions.csv', header=None)
chenDu.columns = ['name', 'id','NorP','bactOrFungi','nucID']
## our proteins are here:
prots = chenDu[chenDu['NorP'] == 'p'].id.to_list()
## remove problematic protein sequence:
prots.remove('WP_011873136.1')

## find out which of the ChenDu sequences have an exact match
## to our primers:

hits = []
desc = []
for i in os.listdir('ChenDuAccs'):
    print(i)
    aa = SeqIO.parse(open(('ChenDuAccs/'+i), mode='r'), 'fasta')
    rec = list(aa)[0]
    if KS2Fregex.search(str(rec.seq)): hits.append(i); desc.append(rec.description)
    if KS2Rregex.search(str(rec.seq)): hits.append(i); desc.append(rec.description)
    if KS2Frcregex.search(str(rec.seq)): hits.append(i); desc.append(rec.description)
    if KS2Rrcregex.search(str(rec.seq)): hits.append(i); desc.append(rec.description)
    hits = [ i.replace('.fa', '') for i in hits ]
    hitsU = list(set(hits))


hasPrim = chenDu[chenDu['nucID'].isin(hitsU)]
hasPrim.reset_index(inplace=True, drop=True)
hasPrim ## these are our accessions of interest

## our files therefore are:
hasPrimerFiles = [ ("ChenDuAccs/"+i+".fa") for i in hasPrim.nucID.values ]

seqName=(hasPrim.nucID.values[i] + '_primer-' +'KS2F')
fastafile=hasPrimerFiles[i]

primer=KS2Fregex

accessionseqs=[]
for fileNu, file in enumerate(hasPrimerFiles):
    print(file)
    for primNu, prim in enumerate(primSeries):
        seqName=(hasPrim.nucID.values[fileNu] + '_primer-' +primSeries.index[primNu])
        print(seqName)
        if 'F' in primSeries.index[primNu]: n = (+1)
        elif 'R' in primSeries.index[primNu]: n = (-1)
        primHitSeqs=getSeqsForAPrimer(file, primer=prim, seqName=seqName, seqLength=(500*n))
        accessionseqs += primHitSeqs

SeqIO.write(accessionseqs, 'chenDuPrimerHits.fa', 'fasta')

## hmm, hard to do sanity checks on these. guess we gotta do an alignment and 
## see if it makes any sense at all.

## to BASH

muscle -in chenDuPrimerHits.fa -out testAlignChenduHits.fa

alv testAlignChenduHits.fasta

## anyway, looks promising

## that's probably all the time we have for now to deal with the primers.

######## crane site data #######

## okay, gotta start compiling and understanding the data that is available
## from the iDiv folks.

## what have we got?

## we got tree locations, and species id, and DEM, etc, lots of good stuff. 

## what do we do now?

## nail down experimental methods, and sampling scheme. 

## I think I am proposing two projects: functional microbiome survey and transport 
## of fungi.

## a third is hidden in there, doing a basic fungal taxonomic diversity survey, 
## by tucking ITS work in whereever barcoding happens, but downplay this in the
## grant to avoid appearance of over-committing, and making the application too
## complex.

## for the stemflow, basically accompany the proposed stem flow survey with some sterile 
## plants, both subject to stemflow from tree and protected from tree. 

## seedlings will be of one species, the most common or most commonly sampled tree,
## to control for host effects of community assembly. Thus most of the time the 
## seedling species matches host tree, but sometimes doesn't. 

## can we get our tree species into a geojson and into a gpd df?

import copy, os, re
import pandas as pd 
import numpy as np
import geopandas as gpd
import rasterio
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
plt.ion()

lccTrees = gpd.read_file("/home/daniel/Documents/grants/"
                    "geneClust/data/LCCdata/modifiedGISforApp"
                    "/LCCtrees.geojson")


## what are our most common trees?

lccTrees.head()

lccTrees.tail()

lccTrees.spec

lTgroupSpp = lccTrees.groupby(lccTrees.spec)

abundances = lTgroupSpp.agg(np.size)['Tree_ID'].copy()

abundances.sort_values(ascending=False, inplace=True)

abundances.plot.bar()

abundances

"""
BAH = Acer pseudoplatanus
BUL = Ulmus glabra
FAH = Acer campestre
FEUL = Ulmus minor
FLUL = Ulmus laevis
GES= Fraxinus excelsior
HAS = Corylus avellana
HBU= Carpinus betulus
HOL = Sambusus nigra
KPA = Populus canadensius
REI = Quercus rubra
RES = Fraxinus pensylvanica
RKA = Aesculus hippocastanum
RO= Robinia pseudoacacia
SAH= Acer platanoides
SEI= Quercus robur
VKI = Prunus avium
WD = Crataegus sp. 
WLI = Tilia cordata  
"""

## most common types are two non-native maples and tilia.
## what's the goal here? what did Herrmann do in her last paper?

## they did three species, maple, tilia, and oak
## three individuals from each species
## three zones each (lower, mid, upper)
## then three samples from each tree/zone

## 3 x 3 x 3 = 27 samples

## some problems with the sample, spatially clustered - 
## all the oaks at one end, all the lindens at the 
## other, maples distributed between the two clusters.

## so spatial effects alone could have caused their
## putative host effects. 

## have to fix that. 

## to really test host effects, seedlings would be nice. 

lTgroupSpp.agg

help(lTgroupSpp.agg)

## so if I could have all the miseq runs, money, and time I ever wanted,
## what would I sample?

## 1a - match the old sampling, but correct for spatial problem,
## so do a few trees in the middle, match age class and 
## 1b - check for seasonal and yearly effects. 
## 1c - phenology - for 2 years, 2 sampling events per year (beginning and end of year)

## 2 - functional microbiome surveys, 2 years, 2 sampling events
##      - first year without seedlings, also repeat/match bacterial sampling
##      - environmental gradient: seedlings in exposed and sheltered conditions, predicted elevated melanin, anthraquinones, other polyketide pigments:
##      - pertinent secondary metabolites? 

## 3 - stemflow traps - check filters for fungi, also place 

## first year:

## conduct functional study
## grow seedlings
## conduct base level ITS surveys 

## second year:

## seedling studies

## seedings - each site gets 

## 1 open to litter, closed to stem flow (+/-)
## 2 closed to litter, open to stem flow (-/+)
## 3 closed to litter, closed to stem flow (-/-)
## 4 open to litter, open to stem flow  (+/+)

## so let's frame this as a general investigation of the functional 
## microbiome, with an obersvational phase and a experimental phase
## this first phase would also allow the testing of the primers. 

## first year would repeat the methods of Hermann 2020, plus a few 
## trees, but with ITS and candidate PKS/NRPS primers.  Second year 
## would use seedlings, and implement stemflow setup. 

## redo sample diagram to have three zones. 

## make map, with additonal trees. 

## diagram of stem flow experimental setup with seedlings

## first year - 18 samples / tree, 


## anyway, let's make a map of the three species that Herrmann used

lccTrees.head()

## our three species are:

herrmannSpec = ['BAH','SEI','WLI']

## subset to just these:

justHerrmannSpec = lccTrees[lccTrees.spec.isin(herrmannSpec)]

## still huge, about half of our species
justHerrmannSpec.shape

## plot, colored by species.

## get our colors

colorDic = {'BAH':'#f19c43',
            'SEI':'#549ad7',
            'WLI':'#3c7403'}
colsSpec = justHerrmannSpec.spec.apply(lambda x: colorDic[x])

fig, ax = plt.subplots()
justHerrmannSpec.plot(c=colsSpec, ax=ax)

## can we identify the trees used in the original study?:

herrmannTrees = [
                "K129.0",
                "K317.0",
                "K733.0",
                "K33.0" ,
                "K513.1",
                "K754.0",
                "K444.0",
                "K455.0",
                "K517.1"
                ]

herrmannTrees = justHerrmannSpec.Tree_ID.isin(herrmannTrees)

justHerrmannTrees = justHerrmannSpec[herrmannTrees]

colsSpec = justHerrmannTrees.spec.apply(lambda x: colorDic[x])

fig, ax = plt.subplots()
justHerrmannTrees.plot(c=colsSpec, ax=ax)

## well, that looks a little different than their report. 

## less problematic, spatially. I wonder which is correct?

## anyway, doesn't matter. What do we need here? 

## this just in, the Herrmann lab group has decided to drop
## maples, and sample ash instead

## and here is her summary:
## We plan to sample two individuals of three tree species
## each will be equipped with triplicate throughfall samplers at three different height levels (top, mid, bottom position). 
## That will be 9 throughfall samplers per tree. 
## 6 trees x 9 samplers = 54 throughfall samplers total.
## which will give us insight into carbon and nitrogen compounds and microorganisms being transported via throughfall. 
## We were planning to sample six times a year focusing mostly on the vegetation period. 
## Out of these samples, we will select three or four time points a year for in depth community analysis. 
## Each time we sample throughfall, we will also sample leave material nearby the throughfall samplers. 
## Phyllosphere microbial communities will be analyzed for those time points for which we also analyze the communities in the throughfall. 
## We plan to follow this sampling design for two vegetation periods (2021 and 2022). 

## so what am I adding to this?

## ITS sampling to every 16s survey
## seedlings. 

## diagram for network model of leaf collections

## do we have rain data?

wd = ("/home/daniel/Documents/grants/geneClust/data/LCCdata/climateData/")
d4pd = dict()
for h,i in enumerate(os.listdir(wd)):
    print(i)
    aa = pd.read_csv(wd+i)
    bb = aa[['Date','RegenSumme in mm']].copy()
    bb['month'] = [ j[0] for j in bb.Date.str.split("/") ]
    bb.drop(columns='Date', inplace=True)
    cc = bb[['month', 'RegenSumme in mm']].copy()
    yr = re.compile('20[0-9][0-9]')
    yearStr = yr.findall(i)[0]
    print('yearStr = ' + yearStr)
    newnames = list(cc.columns)
    newnames[1] = ('Rain' + yearStr)
    cc.columns = newnames
    dd = pd.Series(cc[('Rain' + yearStr)])
    dd.set_index = cc['month']
    d4pd[('Rain' + yearStr)] = dd 

d4pd['month'] = bb['month']
rainDF = pd.DataFrame(d4pd)
rainDF.set_index('month', inplace=True)
rainDF = rainDF[rainDF.columns.sort_values()]

rainDF.plot()

## that took forever, but the basic message is that there is plenty of summer water 

## great, lots of water, no worries about the stemflow stopping during the summer

## what else? map of plots  

## I think we can assume that they will situate the stemflow on a tree that already 
## has sensors. 

## and they changed the trees that they want to work, so let's see how much we can 
## narrow this down:

import pandas as pd 
import numpy as np
import geopandas as gpd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
plt.ion()

lccTrees = gpd.read_file("/home/daniel/Documents/grants/"
                    "geneClust/data/LCCdata/modifiedGISforApp"
                    "/LCCtrees.geojson")

lccSensors = gpd.read_file("/home/daniel/Documents/grants/"
                    "geneClust/data/LCCdata/modifiedGISforApp"
                    "/LCCSensors.geojson")

##  clear up the LI/WLI difference
lccSensors['BAUMART'] = lccSensors.BAUMART.str.replace('LI','WLI')

## make the unique IDs compatible:
lccTrees['Tree_ID'] = lccTrees['Tree_ID'].str.replace('K','')


## let's see if we can predict the trees by narrowing it down:

## our three species are now:
herrmannSpec = ['GES','SEI','WLI']

## subset to just these:

justHerrmannSpec = lccTrees[lccTrees.spec.isin(herrmannSpec)]

colorDic = {'BAH':'#f19c43',
            'GES':'#861c83',
            'SEI':'#549ad7',
            'WLI':'#3c7403'}
colsSpec = justHerrmannSpec.spec.apply(lambda x: colorDic[x])


fig, ax = plt.subplots()
justHerrmannSpec.plot(c=colsSpec, ax=ax)

## okay, now which ones have sensors?

lccSensors.head()

lccSensors.tail()

## we need the unique tree IDs:

lccSensors.Name

herrmTreeIDs = justHerrmannSpec['Tree_ID'].str.replace('K','')
sensTrees = herrmTreeIDs.isin(lccSensors.Name)

justHerrmSpecSens = justHerrmannSpec[sensTrees] 

## plot it
colsSpec = justHerrmSpecSens.spec.apply(lambda x: colorDic[x])
fig, ax = plt.subplots()
justHerrmSpecSens.plot(c=colsSpec, ax=ax)

## we could narrow down to the species that were sampled previously:

herrmannOldTrees = [
                "129.0",
                "317.0",
                "733.0",
                "33.0" ,
                "513.1",
                "754.0",
                "444.0",
                "455.0",
                "517.1"
                ]

oldTreesFilt = lccTrees.Tree_ID.isin(herrmannOldTrees)
oldNewTrees = lccTrees[oldTreesFilt]


## plot it
colsSpec = oldNewTrees.spec.apply(lambda x: colorDic[x])

fig, ax = plt.subplots()
oldNewTrees.plot(c=colsSpec, ax=ax)

## scale by girth?

oldNewTrees[['Tree_ID', 'BHD_2020']]

BHDmarker = (oldNewTrees.BHD_2020
                .str.replace(',','.')
                .astype('float64'))

## or maybe height?

HGmarker = (oldNewTrees.hG_2020
                .str.replace(',','.')
                .astype('float64'))

colsSpec = oldNewTrees.spec.apply(lambda x: colorDic[x])

fig, ax = plt.subplots()
oldNewTrees.plot(c=colsSpec, ax=ax, markersize = BHDmarker)

fig, ax = plt.subplots()
oldNewTrees.plot(c=colsSpec, ax=ax, markersize = HGmarker)

## diameter is more useful for distinguishing these.

## do the heights agree between the sensor and the general 2020 data?

## anyway, those five trees are likely candidates for 
## Herrmann's next study


## so looks like we would want trees that are: 

## ash
## 30-40m tall, 
## have sensors
## have non-clustered distribution
## near our other sampled trees? seems like you want to do some sort of neutral spatial effects analysis.

## are there many of these?:

lccTrees


## goal here is to present a map to the group. maybe use bokeh 
## because it is fun.

## answer my own question, so to speak.

lccSensors.Name

lTID = lccTrees['Tree_ID'].str.replace('K','')

lccTrees[lTID.isin(lccSensors.Name)] ## only 28 trees with sensors, is this right?

## no, missing two sensor trees. Just work with the sensor data

## back to our checklist:

## 1) have sensors
## 2) ash
## 3) 30-40m tall, 
## 4) have non-clustered distribution
## 5?) near our other sampled trees? seems like you want to do some sort of neutral spatial effects analysis.

## 1 & 2 - subset sensor Trees to ash:
ashSensFilt = (lccSensors.BAUMART == 'GES')

ashSens = lccSensors[ashSensFilt]

## 3) 30-40m tall. The lccSensor height looks a little wonky to me. 
##    So get it from the main spreadsheet data:
## So grab it from the main spreadsheet data

def getHeight(name):
    aa = (lccTrees[lccTrees['Tree_ID'] == name]
            .hG_2020.values[0]
            .replace(',','.')
            )
    return(aa)

ashSens.Name.apply(getHeight) ## all between 32m and 37m, so no refinement here

## while we're at it, get BHD for plotting


def getBHD(name):
    aa = (lccTrees[lccTrees['Tree_ID'] == name]
            .BHD_2020.values[0]
            .replace(',','.')
            )
    return(aa)

ashBHD = ashSens.Name.apply(getBHD).astype('float64')

## according to this they all fit, with maybe the first being too tall. 

## 4 & 5) non-clustered, near other trees. Just look at them and 
## pick out a few:

ashColsSpec = ashSens.BAUMART.apply(lambda x: colorDic[x])
fig, ax = plt.subplots()
oldNewTrees.plot(c=colsSpec, ax=ax, markersize = BHDmarker)
ashSens.plot(c=ashColsSpec, ax=ax, markersize = ashBHD)

handles, labels = ax.get_legend_handles_labels()
ax.legend(handles, labels)


## let's make a bokeh plot

## all trees, sized by DBH
## if not of interest, faded gray
## with hover id and species
## of interest: sensors present, ashes, and trees from previous study, excluding maples

import geopandas as gpd
import pandas as pd
import numpy as np
from bokeh.io import show, output_notebook, output_file
from bokeh.models import (ColumnDataSource,
                            HoverTool)
from bokeh.plotting import figure


### data for bokeh plot ###
## GIS from Ron
lccTrees = gpd.read_file("/home/daniel/Documents/grants/"
                    "geneClust/data/LCCdata/modifiedGISforApp"
                    "/LCCtrees.geojson")
lccSensors = gpd.read_file("/home/daniel/Documents/grants/"
                    "geneClust/data/LCCdata/modifiedGISforApp"
                    "/LCCSensors.geojson")
## Herrmann et al. sampled these trees before:
prevTrees = [ "129.0", "317.0", "733.0", "33.0" , "513.1", "754.0",
                "444.0", "455.0", "517.1" ]
##  clear up the LI/WLI difference
lccSensors['BAUMART'] = lccSensors.BAUMART.str.replace('LI','WLI')
## make the unique IDs compatible:
lccTrees['Tree_ID'] = lccTrees['Tree_ID'].str.replace('K','')
## make BHD into numeric column
lccTrees['BHD_2020'] = lccTrees['BHD_2020'].replace('na', np.NaN)
lccTrees['BHD_2020'] = lccTrees['BHD_2020'].str.replace(',','.')
lccTrees['BHD_2020'].dropna(axis='index', how='any', inplace=True)
lccTrees['BHD_2020'] = lccTrees['BHD_2020'].astype('float64')
## let's make a Series for markersizes from BHD (they are too big
## as is)
lccTrees['BHDmarkersize'] = lccTrees['BHD_2020'].apply(np.log10)
## we need a separate data source for sensors, I think:
## ring size for sensors:
lccTrees['sensorSize'] = lccTrees['BHDmarkersize'].apply(lambda a: a*1.4)
## sensor df for CDS
sensfilt = lccTrees.Tree_ID.isin(lccSensors.Name)
sensDF = lccTrees.loc[sensfilt,:]
## future trees are most likely trees that:
## aren't maple, are previously-sampled, and have sensors.
## need a list of names that are all the above:
aa = lccTrees[~lccTrees['spec'].isin(['BAH'])].copy()
bb = aa[aa['Tree_ID'].isin(prevTrees)].copy()
cc = bb[bb.Tree_ID.isin(lccSensors.Name)].copy()
cc.drop('geometry',axis='columns', inplace=True)
mostLikelySampleAgainDF = cc
## and now the ashes with sensors on them:
aa = lccTrees[lccTrees['spec'].isin(['GES'])].copy()
bb = aa[aa.Tree_ID.isin(lccSensors.Name)].copy()
bb.drop('geometry',axis='columns', inplace=True)
mostLikelyAshesDF = bb
sensorCDS = ColumnDataSource(data=sensDF.drop('geometry', axis='columns'))
mostLikelySampleAgainCDS = ColumnDataSource(data=mostLikelySampleAgainDF)
mostLikelyAshesCDS = ColumnDataSource(data=mostLikelyAshesDF)
lccCDS = ColumnDataSource(data=lccTrees.drop('geometry', axis='columns'))
output_file('lccTrees2.html')
p = figure (title='LCC trees ',
           plot_height=600,
            aspect_ratio=1,
            toolbar_location='below',
            tools='pan, wheel_zoom, box_zoom, reset'
           )
## laydown sensor circles
sensorCircs = p.circle('E_UTM', 'N_UTM', source=sensorCDS,
                    fill_alpha=0,
                    line_color='#0000FF',
                    line_width=3,
                    line_alpha=1,
                    radius='sensorSize',
                    legend='sensor',
                    )
## background tree circles
trCircs = p.circle('E_UTM', 'N_UTM', source=lccCDS,
                    fill_color='#a7a7a7',
                    radius='BHDmarkersize',
                    alpha=0.3,
                    )
## non-maple, previously-sampled, probably soon-to-be sampled trees:
mlsaCircs = p.circle('E_UTM', 'N_UTM', source=mostLikelySampleAgainCDS,
                    fill_color='red',
                    radius='BHDmarkersize',
                    alpha=1,
                    legend='probable resample'
                    )
## ashes with sensors:
ashCircs = p.circle('E_UTM', 'N_UTM', source=mostLikelyAshesCDS,
                    fill_color='yellow',
                    radius='BHDmarkersize',
                    alpha=1,
                    legend='ash trees with sensor'
                    )
# Create hover tool
p.add_tools(HoverTool(renderers = [trCircs], 
                      tooltips = [('K','@Tree_ID'),
                                  ('spec','@spec'),]))
show(p)

## works, put it on a notebook, get it out

## that was neat, now what?


############# terhonen lab ##################

## well, shift gears a bit. Now we are potentially working
## with the Eeva Terhonen lab

## Seems like this application will never end.  

## anyway, Dr. Terhonen has some really interesting ideas and data

## first step, can we look her otu tables in python?

import pandas as pd 
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import numpy as np
import networkx, random
from networkx.algorithms import community
import openpyxl
from bokeh.models import MultiLine, Circle, Plot
from bokeh.io import output_file, show
from bokeh.plotting import figure, from_networkx
from matplotlib_venn import venn2
plt.ion()

aa = pd.read_excel('Real_Abundance.xlsx')

## nope. how about
wb = openpyxl.load_workbook('Real_Abundance.xlsx')

wb.sheetnames

raw = wb['Sheet1']

raw.max_row

raw.max_column

## 1270 otus, fits the results listed by the paper. Not sure how they 
## variance stabilized, etc. But not my problem (yet). what do we need
## to do to get this into a useful format for networkx and cooccur?

## for starters, save out to useable format

eevaOTU = pd.DataFrame(raw.values)
eevaOTU.columns = eevaOTU.iloc[0]
eevaOTU = eevaOTU.drop([0]).reset_index(drop=True)

eevaOTU.to_csv('eevaOTU.csv')

## and what do we want to do with this? 

## we want to (1) run through cooccur pipeline, get an adjacency matrix
## (2) visualize with a bokeh on a notebook 
## (3) run some network diagnostics on it (modularity, nestedness)

## for #2, here's the beta: 
## https://docs.bokeh.org/en/1.3.2/docs/user_guide/graph.html

## so to put this through the cooccur pipeline, I think we want 
## to just do a blunt force comparison between symptomatic and 
## asymptomatic. do we have the sample size for this?

## how many disease class = 0 are there?

len(eevaOTU['SampleID'].unique())

aa = eevaOTU.groupby('Diseaseclass')
## how many are there?

aa.agg(len).SampleID 
## 0    16
## 1    16
## 2    19
## 3    17
## 4    14
## 5    13

## not great, we probably can't afford to subset any more. 
## the simplest comparison here would be group 0, vs. all 
## the rest. 

## maybe 0-2, vs. 3-5  
aa.groups

lowDisSamps = aa.groups[0].append(aa.groups[1]).append(aa.groups[2])
highDisSamps = aa.groups[3].append(aa.groups[4]).append(aa.groups[5])


eevaOTU.iloc[lowDisSamps]

eevaOTU.iloc[highDisSamps]

## we need a new column, hi or low disease. Actually, two dummy 
## variables, I think.

lowDisease = eevaOTU['Diseaseclass'].isin([0,1,2]).replace({True:1, False:0})
highDisease = eevaOTU['Diseaseclass'].isin([3,4,5]).replace({True:1, False:0})

eevaOTU.insert(4,"lowDisease",lowDisease)
eevaOTU.insert(5,"highDisease",highDisease)

## the only other interesting variable I see for this is the sampling 
## time - perhaps certain fungi are more likely to be found in spring 
## than fall? There is some discussion of this in the text. For the 
## moment, let's leave it alone and just focus on the low/high disease
## variable, get rid of everything else:

lowHiDiseaseTable = eevaOTU.drop(columns=['SampleID','Samplingtime','Growthyear','Diseaseclass'])

## I do not trust these abundances. I have no mock community, no raw reads, little
## idea of their variance stabilization methods, etc. And I just don't trust 
## illumina read abundances. 

## so let's go with presence/absence:

lowHiDiseaseTable.head()

(lowHiDiseaseTable > 0)

## ugh, something isn't right. it is finding strings somewhere

for name,i in lowHiDiseaseTable.iterrows():
    bb = i.apply(lambda x: isinstance(x, str))
    if not (i[bb]).empty: 
        print(i[bb])
        print(name)

## seems like just one value, OTU4, row4

lowHiDiseaseTable.loc[4,'OTU4']

## sample 12C, 5th row, OTU4, has three values? weird. but all non-negative.
## so let's give it a value of 1, since we are about to do P/A anyway:

lowHiDiseaseTable.loc[4,'OTU4'] = 1

## and try again:

lowHiDiseaseTablePA = (lowHiDiseaseTable > 0).astype(int)

## can we get rid of the name for our index, it's meaningless, 
## causes problems:

lowHiDiseaseTablePA.columns.name = None


lowHiDiseaseTablePA.head()

## I think we can use this to make our cooccurrence matrix

lowHiDiseaseTablePA.to_csv('lowHiDiseaseTablePA.csv')

## and go to R/cooccur:

library('cooccur')
## I think we need OTUs as rows, sites as columns:
lowHiDiseaseTablePA <- t(read.csv('lowHiDiseaseTablePA.csv', 
                        header=TRUE, row.names=1))

cooccur.sph <- cooccur(mat=lowHiDiseaseTablePA,
                          type="spp_site",
                          thresh=TRUE,
                          spp_names=TRUE)


## that takes a very long time. I miss having access to a 
## computing cluster. 
## plug in power and see if we can 
## sleep.  

## oh jeeze, five hours later still going. 
## I'm betting I can do this the old-fashioned way without
## maxing out my computer, run GLMs on all possible 
## pairs. But it will probably take about as long. 

## leave it for a couple more hours. Hope this pays off, 
## really hope I set it up right. 

## still moving. as of this writing, 83%. 

## There are lots of other ways to make comparisions and 
## construct networks. 

## anyway, stick with it. Assuming that worked, I think we will also want 
## to rerun this, individually, with the early and late disease 
## networks separately. 

## the R code should look like this:

####################################################

## low-disease network:

library('cooccur')

## I think we need OTUs as rows, sites as columns:
lowDiseaseTablePA <- t(read.csv('lowDiseaseTablePA.csv', 
                        header=TRUE, row.names=1))

cooccur.sph.low <- cooccur(mat=lowDiseaseTablePA,
                          type="spp_site",
                          thresh=TRUE,
                          spp_names=TRUE)

save(cooccur.sph.low, file='cooccur.sph.low.rda')

####################################################

## high-disease network:

library('cooccur')

## I think we need OTUs as rows, sites as columns:
highDiseaseTablePA <- t(read.csv('highDiseaseTablePA.csv', 
                        header=TRUE, row.names=1))

cooccur.sph.high <- cooccur(mat=highDiseaseTablePA,
                          type="spp_site",
                          thresh=TRUE,
                          spp_names=TRUE)

save(cooccur.sph.high, file='cooccur.sph.high.rda')

####################################################

## let's see if we can one of these running before going 
## to pickup kiddo. We need to go back to python
## and do some subsetting. 

## to create 'lowDiseaseTablePA.csv'

lowMask = (lowHiDiseaseTablePA['lowDisease'] == 1)
lowDiseaseTablePA = lowHiDiseaseTablePA[lowMask].drop(columns=['lowDisease', 'highDisease'])
## any lost taxa?
(lowDiseaseTablePA.sum(axis=0) == 0).any() ## yes, some taxa are lost
lostTaxa = (lowDiseaseTablePA.sum(axis=0) == 0)
notLostTaxa = ~(lostTaxa)
lowDiseaseTablePA.loc[:,lostTaxa].shape
lowDiseaseTablePA.loc[:,notLostTaxa].shape
## any all zero sites?
(lowDiseaseTablePA.sum(axis=1) == 0).any() ## nope
## save:
lowDiseaseTablePA.to_csv('lowDiseaseTablePA.csv')

## try this out on the R code above for low disease table

###### high disease table 

## rinse and repeat ##
  
highMask = (lowHiDiseaseTablePA['highDisease'] == 1)
highDiseaseTablePA = lowHiDiseaseTablePA[highMask].drop(columns=['lowDisease', 'highDisease'])
## any lost taxa?
(highDiseaseTablePA.sum(axis=0) == 0).any() ## yes, some taxa are lost
lostTaxa = (highDiseaseTablePA.sum(axis=0) == 0)
notLostTaxa = ~(lostTaxa)
highDiseaseTablePA.loc[:,lostTaxa].shape
highDiseaseTablePA.loc[:,notLostTaxa].shape
## any all zero sites?
(highDiseaseTablePA.sum(axis=1) == 0).any() ## nope
## save:
highDiseaseTablePA.to_csv('highDiseaseTablePA.csv')

## and run the above cooccur code on this...all night...

## okay, now what? We need an adjaceny matrix...
## stay in R for a moment. 

## right now we have highDisease network up...

## goal today is to get a visualization pipeline, in bokeh
## tomorrow network diagnostics?

cooccur.sph.high

str(cooccur.sph.high)

head(cooccur.sph.high$results)

cooccur.sph.high$results

tail(cooccur.sph.high$results)

## can we put an FDR=0.5 on this? 

## as usual, I am a little confused, if we want to 
## positive cooccurrence interactions, I think we want 

## for instance:

cooccur.sph.high$results[624486,]

## otu923 and otu1074 are expected to occur 4.4 times,
## but are observed to occur together 8 times, so 
## this is an example of positive cooccurrence. 
## they have a p_gt of 0.01723, and a p_lt of 0.99749
## so it looks like small p_gt are the values to look at 
## for likely cooccurrences, and small l_gt are values
## to look at for negative cooccurrences.  
## and if we want both?

##################################################
## btw, which OTU is sphaeropsis? that could be 
## important/interesting. Have to check!!!

## in the supplementary materials, it's identified
## as OTU3. 

## an interesting question is whether there were 
## any strong negative associations with 
## sphaeropsis
## also we should highlight it and any fungi 
## tightly positively associated with it in our
## diagrams

## problem - sphaeropsis is everywhere.
## so no way to use PA to test this idea
## hmm, how to do this...

##################################################

## first things first, correct the pvalues a bit:

## get our results table out:

cooccur.sph.high.results <- cooccur.sph.high$results 

## apply bejamini-hochburg
p_gt_adj <- p.adjust(cooccur.sph.high.results$p_gt, method = "BH")

length(cooccur.sph.high.results$p_gt)
length(p_gt_adj)

sum(p_gt_adj < 0.05) ## wow, still a lot of cooccurrences

## how much does this correction lose, compared to a p-value=0.5 approach?

sum(cooccur.sph.high.results$p_gt < 0.05) ## oh jeez, 90,000 pos interactions
## so yeah, stay with the simpler, higher confidence network (=BH corrected)

cooccur.sph.high.results$p_gt_adj <- p_gt_adj

## to get a positive cooccurrence network:

posMask <- cooccur.sph.high.results$p_gt_adj <= 0.05

cooccur.sph.high.pos <- cooccur.sph.high.results[posMask,]

max(cooccur.sph.high.pos$p_gt) ## wow...
## that is still a really strict correction, 0.0006 is our highest pval

## how do we turn this into an adjacency matrix? for now, unweighted...

## we can do this R, but I would prefer pandas, so save and head over:
head(cooccur.sph.high.pos )

write.csv(cooccur.sph.high.pos, file="cooccur_sph_high_pos_csv", row.names=FALSE)

## python

highPosCooc = pd.read_csv("cooccur_sph_high_pos_csv")

## since we've already pared down to statistically significant 
## positive cooccurrences, I think we really just need our two 
## columns of species names

aa = highPosCooc.loc[:,['sp1_name','sp2_name']]

## now, how do we splay this out into an adjacency matrix? 
## our columns and index names need to be the union of the two
## columns

bb = highPosCooc.loc[:,'sp1_name']
cc = highPosCooc.loc[:,'sp2_name']

len(bb)
len(bb.unique())

len(cc)
len(cc.unique())

bb.isin(cc).all() ## false
cc.isin(bb).all() ## false

## yeah, so there in otus in the first column that are not in the
## second

dd = bb.append(cc)

len(dd)
len(dd.unique())

## so I think we want this:
otus = (highPosCooc.loc[:,'sp1_name'].
        append(highPosCooc.loc[:,'sp2_name']).
        unique())

matches = highPosCooc.loc[:,['sp1_name','sp2_name']]

## this should be both our column and rownames. 
## now how do we do our pairwise check, to see if they 
## are matched?

## we want a zero matrix with shape of otus x otus:
aa = np.zeros(shape = (len(otus),len(otus)), dtype='int')
## turn this into a pd dataframe
bb = pd.DataFrame(data=aa, index = otus, columns = otus)
## now can we move through our matches object and 
## change 0 to 1 where we have a match?
for _,i in matches.iterrows():
    bb.loc[i.sp1_name,i.sp2_name] = 1

## sanity checks?

bb.to_csv('sanCheck.csv')

bb.loc['OTU595','OTU717']

matches.head()

matches[matches['sp1_name'] == 'OTU595']

matches[matches['sp1_name'] == 'OTU719']

matches[matches['sp1_name'] == 'OTU931']

## think that is okay...
## save this

bb.to_csv('highPosAdjacency.csv')

highPosAdjacency = pd.read_csv('highPosAdjacency.csv', index_col=0)


## will networkx accept this as an adjacency matrix?

adj = networkx.from_pandas_adjacency(highPosAdjacency)

## looks good

print(networkx.info(adj))

layout = networkx.spring_layout(adj)

fig, ax = plt.subplots()
## reuse our colormap 
networkx.draw(adj,
              layout,
              with_labels=False,
              ax=ax)

## shitty graphic. 
## but runs without errors. can we do this with bokeh?

output_file("network.html")
plot = figure(title="Networkx Integration Demonstration", 
                x_range=(-1.1,1.1), 
                y_range=(-1.1,1.1),
                )
graph = from_networkx(adj, networkx.fruchterman_reingold_layout, scale=2, center=(0,0))
graph.edge_renderer.glyph = MultiLine(line_color="#CCCCCC", line_alpha=0.4, line_width=2)
plot.renderers.append(graph)
show(plot)

## then see if we can run some network diagnostics with 
## significance tests. 

cooccur.sph.high.pos

## let's start a notebook for this, eh?
## then sum up pipeline and run for other network, the low disease network.

#### low disease setup and viz  ####

## get our cooccur object 


library('cooccur')
load('cooccur.sph.low.rda')
cooccur.sph.low.results <- cooccur.sph.low$results 
## back up, getting a diagram for the low disease network
p_gt_adj <- p.adjust(cooccur.sph.low.results$p_gt, method = "BH")
cooccur.sph.low.results['p_gt_adj'] <- p_gt_adj
## to get a positive cooccurrence network:
posMask <- cooccur.sph.low.results$p_gt_adj <= 0.05
cooccur.sph.low.pos <- cooccur.sph.low.results[posMask,]
write.csv(cooccur.sph.low.pos, file="cooccur_sph_low_pos.csv", row.names=FALSE)

## to python
lowPosCooc = pd.read_csv("cooccur_sph_low_pos.csv")
otus = (lowPosCooc.loc[:,'sp1_name'].
        append(lowPosCooc.loc[:,'sp2_name']).
        unique())
matches = lowPosCooc.loc[:,['sp1_name','sp2_name']]
aa = np.zeros(shape = (len(otus),len(otus)), dtype='int')
bb = pd.DataFrame(data=aa, index = otus, columns = otus)
for _,i in matches.iterrows():
    bb.loc[i.sp1_name,i.sp2_name] = 1

bb.to_csv('lowPosAdjacency.csv')

lowPosAdjacency = pd.read_csv('lowPosAdjacency.csv', index_col=0)

lowAdj = networkx.from_pandas_adjacency(lowPosAdjacency)
print(networkx.info(lowAdj))

layout = networkx.spring_layout(lowAdj)

fig, ax = plt.subplots()
## reuse our colormap 
networkx.draw(lowAdj,
              layout,
              with_labels=False,
              ax=ax)

## shitty graphic. 
## but runs without errors. can we do this with bokeh?

output_file("network.html")
plot = figure(title="Low disease network", 
                x_range=(-1.1,1.1), 
                y_range=(-1.1,1.1),
                )
graph = from_networkx(lowAdj, networkx.fruchterman_reingold_layout, scale=2, center=(0,0))
graph.edge_renderer.glyph = MultiLine(line_color="#CCCCCC", line_alpha=0.4, line_width=2)
plot.renderers.append(graph)
show(plot)

#### combo disease setup and viz  ####

## get our combo cooccur object 


library('cooccur')

load('cooccur.sph.rda')
cooccur.sph.combo.results <- cooccur.sph$results 
p_gt_adj <- p.adjust(cooccur.sph.combo.results$p_gt, method = "BH")
cooccur.sph.combo.results['p_gt_adj'] <- p_gt_adj
## to get a positive cooccurrence network:
posMask <- cooccur.sph.combo.results$p_gt_adj <= 0.05
cooccur.sph.combo.pos <- cooccur.sph.combo.results[posMask,]
#write.csv(cooccur.sph.combo.pos, file="cooccur_sph_combo_pos_csv", row.names=FALSE)

cooccur.sph.combo.pos.head()

## to python
comboPosCooc = pd.read_csv("cooccur_sph_combo_pos_csv")
otus = (comboPosCooc.loc[:,'sp1_name'].
        append(comboPosCooc.loc[:,'sp2_name']).
        unique())
matches = comboPosCooc.loc[:,['sp1_name','sp2_name']]
aa = np.zeros(shape = (len(otus),len(otus)), dtype='int')
bb = pd.DataFrame(data=aa, index = otus, columns = otus)
for _,i in matches.iterrows():
    bb.loc[i.sp1_name,i.sp2_name] = 1
bb.to_csv('comboPosAdjacency.csv')
comboPosAdjacency = pd.read_csv('comboPosAdjacency.csv', index_col=0)
comboAdj = networkx.from_pandas_adjacency(comboPosAdjacency)
print(networkx.info(comboAdj))

layout = networkx.spring_layout(comboAdj)

fig, ax = plt.subplots()
networkx.draw(comboAdj,
              layout,
              with_labels=False,
              ax=ax)

## shitty graphic. 
## but runs without errors. can we do this with bokeh?
## I think we need this info to be established in the 
## the networkx info. 


output_file("network.html")
plot = figure(title="All sample network", 
                x_range=(-1.1,1.1), 
                y_range=(-1.1,1.1),
                )
graph = from_networkx(comboAdj, networkx.fruchterman_reingold_layout, scale=2, center=(0,0))
graph.edge_renderer.glyph = MultiLine(line_color="#CCCCCC", line_alpha=0.4, line_width=2)
plot.renderers.append(graph)
show(plot)



## we need to be able to identify specific nodes and highlight their associations
## specifically to get a different node color for OTU3, sphaeropsis, 
## and for the high/low nodes, if present.
## how do we do this?

cooccur_combo_pos = pd.read_csv("cooccur_sph_combo_pos_csv")

cooccur_combo_pos.head()

cooccur_combo_pos.sp1_name.isin(['OTU3']).any()
cooccur_combo_pos.sp2_name.isin(['OTU3']).any()
## no sign of sphaeropsis. To be expected. 

G = networkx.karate_club_graph()

G.edges

graph = from_networkx(comboAdj, networkx.fruchterman_reingold_layout, scale=2, center=(0,0))

normal_color = "gray"
highDiseaseNode_color, lowDiseaseNode_color = "black", "red"
SphaeropsisNode_color = "blue"
node_attrs = {}

for i, _ in comboAdj.nodes(data=True):
    if i == "OTU3":
        node_color = SphaeropsisNode_color
    elif i == "lowDisease":
        node_color = lowDiseaseNode_color
    elif i == "highDisease":
        node_color = highDiseaseNode_color
    else:
        node_color = normal_color
    node_attrs[i] = node_color

networkx.set_node_attributes(comboAdj, node_attrs, "node_color")

###

node_attrs

plot = figure(title="All sample network", 
                x_range=(-1.1,1.1), 
                y_range=(-1.1,1.1),
                )
graph = from_networkx(comboAdj, networkx.fruchterman_reingold_layout, scale=2, center=(0,0))
graph.edge_renderer.glyph = MultiLine(line_color="#CCCCCC", line_alpha=0.4, line_width=2)
graph.node_renderer.glyph = Circle(size=15, fill_color="node_color")
plot.renderers.append(graph)

output_file("interactive_graphs.html")
show(plot)

### network diagnostics ###

## we want the following:

# nestedness, check nestedtemp in vegan package:
https://cran.r-project.org/web/packages/vegan/vegan.pdf
# modularity looks simple enough:
https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.quality.modularity.html?highlight=modularity#networkx.algorithms.community.quality.modularity 

# Mean degree <k>
# Degree distribution
https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.degree.html?highlight=networkx%20graph%20degree#networkx.Graph.degree
## Average shortest path length <l>
## just use djisktra?
## Mean clustering coefficient <C>
networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.cluster.average_clustering.html?highlight=mean clustering coefficient
## Betweenness centrality <CB>
## Closeness centrality <CC>
https://networkx.org/documentation/stable/reference/algorithms/centrality.html
## Indicator species for low disease and high-disease
## this one has a built in hypothesis test, could probably run it tonight.

## and we need null graphs for hypothesis tests, I think.
## random graph generators are here:
https://networkx.org/documentation/stable/reference/generators.html#module-networkx.generators.random_graphs
## this might be helpful:
https://ericmjl.github.io/Network-Analysis-Made-Simple/04-advanced/03-stats/
## but that looks computationally really expensive...we'll see I guess. 

## also any otus that are negatively associated with the 
## disease symptoms. This means any species that are 
## positively associated with low disease severity,
## and possibly those that negatively associated with 
## high disease severity. 

## these are probably easily gotten:

### otus associated with low disease severity:

cooccur_combo_pos = pd.read_csv("cooccur_sph_combo_pos.csv")

lowDiseaseAssociateMask = cooccur_combo_pos['sp1_name'] == 'lowDisease' 

lowDisAssociates = cooccur_combo_pos[lowDiseaseAssociateMask].sp2_name

## OTU194  this is an unidentified ascomycete
## OTU726  this is an unidentified fungus, not even to phyla
## OTU993  this is an unidentified ascomycete

## great. So our most promising fungi are all unculturable and unidentifiable.

## just checking, any of our second column of species?
(cooccur_combo_pos['sp2_name'] == 'lowDisease').any()
## nope

### otus negatively associated with high disease severity. 

## okay, conversely, we can look at those that are never found
## with high disease severity. This is even more ambiguous, 
## because they could just be "wimps" that are lost when the
## going gets tough. Or they could be important somehow to 
## plant health. Anyway, check it out:

## back in R, need the negative cooccurrences this time:

head(cooccur.sph.combo.results)

cooccur.sph.combo.results

p_lt_adj <- p.adjust(cooccur.sph.combo.results$p_lt, method = "BH")

cooccur.sph.combo.results['p_lt_adj'] <- p_lt_adj

negMask <- cooccur.sph.combo.results$p_lt_adj <= 0.05

aa <- cooccur.sph.combo.results[negMask,]

## and nothing. only the node "lowDisease" is negatively
## associated with anything, and that is "highDisease"

## so negative associations are really found much after
## correcting for multiple tests. 

### positive associations with high disease state

## finally, just curious, are there other species 
## associated with the high disease state besides 
## sphaeropsis?

cooccur_combo_pos = pd.read_csv("cooccur_sph_combo_pos.csv")

highDiseaseAssociateMask = cooccur_combo_pos['sp1_name'] == 'highDisease' 

highDisAssociates = cooccur_combo_pos[highDiseaseAssociateMask].sp2_name

## OTU709 Phaeosphaeria sp., So another pathogen? 
## OTU343 Fungus sp., basically unidentified

### indicator species ###

## we can check our disease classes for indicator species, also...
## seems like the best way to do this is to do by diease 
## classes?

## in R:
library(indicspecies)

## our community matrix will be the all OTU-table, minus
## the treatment columns

aa <- read.csv('lowHiDiseaseTablePA.csv')
aa <- aa[,4:ncol(aa)]

bb <-read.csv('eevaOTU.csv')$Diseaseclass


## aa is our community matrix, but I think we need to transpose it?
## actually doesn't look like it

## what indicators do we want to hunt down?

## start with all disease classes. It should naturally
## find our low disease (0-2) and high disease (3-5) 
## combinations without us asking it to, with the 
## multipatt command:

habIndSpp <- multipatt(aa, bb, func = 'r.g', control=how(nperm=9999))

diseaseClassIndSpp <- habIndSpp

save(diseaseClassIndSpp, file='diseaseClassIndSpp.rda')

## using r.g function includes a correction for multiple comparisons,
## so don't go farther with a BH.

load('diseaseClassIndSpp.rda')

str(diseaseClassIndSpp)

summary(diseaseClassIndSpp)

diseaseClassIndSpp

cc <- diseaseClassIndSpp$sign

cc[cc$p.value <= 0.05,]

dd <- cc[cc$p.value <= 0.08,]

ee <- dd[complete.cases(dd),]

summary(diseaseClassIndSpp)

### NMS of disease class ###

## we have to be strategic, however. I'm not trying to publish a 
## paper (yet), I'm trying to write a grant proposal. I also promised 
## to deliver some interesting network diagnostics. 

## In just a few days...

## anyway, noisy data, how do the NMSes look?

library(vegan)
library(RColorBrewer)

nms <- metaMDS(aa, try=40)

stressplot(nms)

## wow, that actually looks really good, stress =.10, strong R2

## okay, how do we plot the disease groups?

eevaOTU <- read.csv('eevaOTU.csv')
colrs <- brewer.pal(6, "Set1")
nmsInfo <- data.frame(XX=numeric(length=95),YY=numeric(length=95))
nmsInfo$XX <- nms$points[,'MDS1']
nmsInfo$YY <- nms$points[,'MDS2']
nmsInfo$diseaseColor <- colrs[eevaOTU$Diseaseclass+1]
nmsInfo$diseaseClass <- eevaOTU$Diseaseclass


par(mfrow=c(1,2))
#par(mfrow=c(1,1))
plot(nmsInfo$XX, nmsInfo$YY,
    col=nmsInfo$diseaseColor,
    pch=19,
    cex=2.0,
)
ordihull(nms, 
    nmsInfo$diseaseColor, 
    #nmsInfo$diseaseClass, 
    #show.groups=c(1,6),
    col=names(table(nmsInfo$diseaseColor)),
    lwd=3,
)
orderCs <- c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3", "#FF7F00", "#FFFF33")
legend( x='bottomleft',
        legend = 0:5,
        fill = orderCs,
        col = orderCs,
        cex = 1.0,
        )
text(nmsInfo$XX, nmsInfo$YY, nmsInfo$diseaseClass)
plot(nmsInfo$XX, nmsInfo$YY,
    col=nmsInfo$diseaseColor,
    pch=19,
    cex=2.0,
    xlim=c(-0.4,0.0),
    ylim=c(-0.2,0.2),
)
ordihull(nms, 
    nmsInfo$diseaseColor, 
    #show.groups=c(1,6),
    col=names(table(nmsInfo$diseaseColor)),
    lwd=3,
)
## add a legend
legend( x='bottomleft',
        legend = 0:5,
        fill = orderCs,
        col = orderCs,
        cex = 1.0,
        )
text(nmsInfo$XX, nmsInfo$YY, nmsInfo$diseaseClass)

## did that work? 

names(table(nmsInfo$diseaseColor))


## and if we just group by high/low disease?

par(mfrow=c(1,1))
plot(nmsInfo$XX, nmsInfo$YY,
    col=nmsInfo$diseaseColor,
    pch=19,
    cex=2.0,
)
#hulls <- ordihull(nms, 
#    nmsInfo$diseaseColor, 
#    #nmsInfo$diseaseClass, 
#    #show.groups=c(1,6),
#    col=names(table(nmsInfo$diseaseColor)),
#    lwd=3,
)
hulls <- ordihull(nms, 
    nmsInfo$diseaseColor, 
    #nmsInfo$diseaseClass, 
    #show.groups=c(1,6),
    col=names(table(nmsInfo$diseaseColor)),
    lwd=3,
)
orderCs <- c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3", "#FF7F00", "#FFFF33")
legend( x='bottomleft',
        legend = 0:5,
        fill = orderCs,
        col = orderCs,
        cex = 1.0,
        )
text(nmsInfo$XX, nmsInfo$YY, nmsInfo$diseaseClass)
points(summary(hulls)['NMDS1',],summary(hulls)['NMDS2',], col=colnames(summary(hulls)), pch=19, cex=10, bg=colnames(summary(hulls)))

## not a clear story, because class 3 and 4 don't behave like you would expect.
## not 

## does the permanova say anything about this?

aa <- read.csv('lowHiDiseaseTablePA.csv')
lowhi <- aa$lowDisease
aa <- aa[,4:ncol(aa)]
bb <-read.csv('eevaOTU.csv')$Diseaseclass
permDC <- adonis(aa ~ bb)
permLH <- adonis(aa ~ lowhi)

## what if we subset to just class 0 and class 5?

onlyGoodBadFilter = bb == 0 | bb == 5 

onlyGoodBad <- aa[onlyGoodBadFilter,]

onesFives <- bb[onlyGoodBadFilter]

permGoodBad <- adonis(onlyGoodBad ~ onesFives)


## just to check, rda:

spp=decostand(aa,method = "hellinger")#Convert response variables
uu=rda(spp,bb)#RDA Analysis
anova(uu)

## well, almost statistically significant (p=0.06) but only explaining 
## like 0.5% of the variation in the data.

## and just the 0s and 5s?
spp=decostand(onlyGoodBad,method = "hellinger")#Convert response variables
uu=rda(onlyGoodBad,onesFives)#RDA Analysis
anova(uu)

## huh, not much to work with there. 
## too noisy. I can stare at the NMS and imagine 
## some patterns, but not going to get anywhere 
## with this right now.  
## We have indicator species and 
## cooccurrence 


## so to finish up, we need to look at the network architecture. 
## long night on this, probably. 

## back to networkx in python

##### network stats ####

## okay, start over, get our network objects into place

## can we get all of the above metrics for our two networks?

## what are our null hypotheses? 

## what are the chances of getting the same statistic, in 
## a network with the same number of nodes, same number of 
## edges, but randomly distributed edges?

## our lowDisease network:
lowPosAdjacency = pd.read_csv('lowPosAdjacency.csv', index_col=0)
lowAdj = networkx.from_pandas_adjacency(lowPosAdjacency)

## our highDisease network:
highPosAdjacency = pd.read_csv('highPosAdjacency.csv', index_col=0)
highAdj = networkx.from_pandas_adjacency(highPosAdjacency)

### nestedness

## um, where to do this?
## there is a package that proposes to do this in R (MBI):
## the paper for the index wnodf is:

## Almeida-Neto M, Ulrich W (2011) A straightforward computational approach for measuring nest-edness using quantitative matrices. Environmental Modelling and Software, 26, 173-178

## in R:

library(MBI)

lowHiPA <- read.csv('lowHiDiseaseTablePA.csv')
lowPA <- lowHiPA[lowHiPA$lowDisease == 1,4:ncol(lowHiPA)]
highPA <- lowHiPA[lowHiPA$highDisease == 1,4:ncol(lowHiPA)]

wnodf(lowPA) ## 38.33365
#lowPawnodf = wnodf(lowPA) ## 38.33365
highPawnodf = wnodf(highPA) ## 43.06346 

highPawnodf$row

as.character(highPawnodf$row)

print("low disease nestedness (WNODF) = " + str(wnodf(lowPA)))

print(paste("high disease nestedness (WNODF) =",as.character(highPawnodf$row)))

## so slightly higher nestedness, but not dramatic. 

### modularity 
## have to get communities first. Not sure what algorithm works best...
## try the "Clauset-Newman-Moore greedy modularity maximization" algorithm:

lowAdj_communities_generator = community.greedy_modularity_communities(lowAdj)
lowAdj_communities = list(lowAdj_communities_generator)

len(lowAdj_communities)
## 18 communities in low
[ len(i) for i in lowAdj_communities ]



highAdj_communities_generator = community.greedy_modularity_communities(highAdj)
highAdj_communities = list(highAdj_communities_generator)

len(highAdj_communities)
## 24 communities in high
[ len(i) for i in highAdj_communities ]


plt.close('all')
fig, axes = plt.subplots(2)
axes[0].bar(range(0,len(lowAdj_communities)), [ len(i) for i in lowAdj_communities ], color='blue')
axes[0].set_title('Communities/Modules in low-disease samples')
axes[1].bar(range(0,len(highAdj_communities)), [ len(i) for i in highAdj_communities ], color='red')
axes[1].set_title('Communities/Modules in high-disease samples')
plt.tight_layout()



[ i in lowAdj_communities for i in highAdj_communities ]

## are any of these communities exactly the same?

[ i in highAdj_communities for i in lowAdj_communities ]
[ i in lowAdj_communities for i in highAdj_communities ]
## nope. not surprising.

## I think we can use these to measure modularity:
aa = community.modularity(lowAdj, lowAdj_communities)
## 0.221

bb = community.modularity(highAdj, highAdj_communities)
## 0.249

## not sure how to decide if these differences are meangingful.

### Mean degree <k>

lowAdj.number_of_edges() / lowAdj.number_of_nodes() * 2
## 29.6

highAdj.number_of_edges() / highAdj.number_of_nodes() * 2
## 21.9

## this is pretty easy, but should stay 
## the same under our null hypothesis, I think,
## because we are going to hold the number of 
## edges and nodes constant. We are just going 
## to mix up the associations, who is tied to who. 

### Degree distribution

fig, ax = plt.subplots(2, sharex=True)
lowAdjdegreez = [ i[1] for i in lowAdj.degree ]
ax[0].hist(lowAdjdegreez, log=False, bins=len(lowAdjdegreez))
ax[0].set_title('low disease network')
highAdjdegreez = [ i[1] for i in highAdj.degree ]
ax[1].hist(highAdjdegreez, log=False, bins=len(highAdjdegreez))
ax[1].set_title('high disease network')
fig.suptitle('degree distributions')

## lowAdj has longer, fatter tail
## looks scale free. is there are way to make a 
## summary stat from this? not sure. don't do that right now

## can we find our most connected nodes? 

lowAdj.degree['OTU250']

aa = list(lowAdj.degree)
bb = list(zip(*aa))
cc = pd.Series(bb[1], index=bb[0])
cc[cc == cc.max()]
lowAdjDeg = cc.sort_values(ascending=False)

## otu250, ## 217 connections
highAdj.degree['OTU250']
## also common in the high-network, with 150 edges

## checking our low-associated fungi
lowAdj.degree['OTU194'] ## only 1 association
lowAdj.degree['OTU726'] ## not in there. Must be another lone wolf or ubiquitous
lowAdj.degree['OTU993'] ## 32 associations, social

aa = list(highAdj.degree)
bb = list(zip(*aa))
cc = pd.Series(bb[1], index=bb[0])
cc[cc == cc.max()]
highAdjDeg = cc.sort_values(ascending=False)
## otu352, 171 edges
lowAdj.degree['OTU352']
## also common in the low-network, with 194 edges

## checking the high-associates:
highAdj.degree['OTU709'] ## nada, lone wolf or ubiquitous. Interesting, this is Phaeospharia
highAdj.degree['OTU343'] ## 5 


highAdj.degree['OTU250']
lowAdj.degree['OTU250']

## in general, can we get the 100 or so topmost-connected OTUS from each 
## network and see how much overlap there is?
## lots to do. not enough time. 

highAdjDeg

highAdjDeg[0:100].index


lowAdjDeg

lowAdjDeg[0:100].index

### Average shortest path length <l>

print(networkx.average_shortest_path_length(lowAdj))
print(networkx.average_shortest_path_length(highAdj))

## these don't work, because there are nodes that are 
## completely isolated from each other, I think.
## i.e. not connected

### Mean clustering coefficient <C>
np.mean(list(networkx.clustering(lowAdj).values()))
np.mean(list(networkx.clustering(highAdj).values()))

### Betweenness centrality <CB>
cbLow = np.mean(list(networkx.betweenness_centrality(lowAdj).values()))
cbHigh = np.mean(list(networkx.betweenness_centrality(highAdj).values()))
print("low disease betweenness centrality = " + str(cbLow))
print("Betweenness centrality = " + str(cbHigh))

### Closeness centrality <CC>
ccLow = np.mean(list(networkx.closeness_centrality(lowAdj).values()))
ccHigh = np.mean(list(networkx.closeness_centrality(highAdj).values()))
print("low disease closeness centrality = " + str(ccLow))
print("high disease closeness centrality = " + str(ccHigh))

### Indicator species for low disease and high-disease
## checked above

#### Random networks / Null dists ####

## how do we develop random networks? 

## one random network with the same number of  
## cooccurrences

## we can use the random graph generator in networkx 

## for our probability edge generation, 

networkx.generators.random_graphs.gnp_random_graph

networkx.generators.random_graphs.gnp_random_graph(

## meh, wait on this. This needs some thought. Useful 
## for publication We need to write something up for 
## Dr. Terhonen. 

## we still have a structured network, in both high and
## low disease cases, but the players in each are 
## changing out a bit. 
## our biggest results are changes in the communities, 
## the differing indicator species

## seems like the best thing to do is to look for a way 
## to illustrate the changes in the communities/modules

## how to do this?

## back to networkx/bokeh...

## we want each community to have its own colors

lowAdj_communities

## okay, so we have two tasks
## 1 turn the community memberships into a single column array 
## that describes each OTU's community
## 2 use this to color our network diagrams

## 1 

lowPosAdjacency = pd.read_csv('lowPosAdjacency.csv', index_col=0)
lowAdj = networkx.from_pandas_adjacency(lowPosAdjacency)

lowAdj_communities

gm,gn = [],[]
for i,j in enumerate(lowAdj_communities):
    gn += [i]*len(j)
    gm += list(j)

gnum = pd.Series(gn, index=gm)
gnum.index.duplicated().any() ## no duplicate OTUs, that's good.

## now we need to embed this group membership as an attribute of a node:

for i in lowAdj.nodes:
    lowAdj.nodes[i]['comGroup'] = gnum[i] 


## we need some colors:
def genXKCDColors(nu):
    Xcolors = random.choices(list(mcolors.XKCD_COLORS.values()), k=nu)
    cmap = mcolors.ListedColormap(Xcolors)
    return(cmap)

colmap = genXKCDColors(len(lowAdj_communities))

node_attrs = {}
for node in lowAdj.nodes(data=False):
    node_color = colmap.colors[lowAdj.nodes[node]['comGroup']]
    node_attrs[node] = node_color

networkx.set_node_attributes(lowAdj, node_attrs, "node_color")

## now use this color attribute with bokeh:

plot = figure(title="Low Disease Network",
                x_range=(-1.1,1.1),
                y_range=(-1.1,1.1),
                )

#graph = from_networkx(lowAdj, networkx.fruchterman_reingold_layout, scale=2, center=(0,0))
graph = from_networkx(lowAdj, networkx.kamada_kawai_layout, scale=2, center=(0,0))
graph.edge_renderer.glyph = MultiLine(line_color="#CCCCCC", line_alpha=0.4, line_width=2)
graph.node_renderer.glyph = Circle(size=15, fill_color="node_color")
plot.renderers.append(graph)
output_file("interactive_graphs.html")
show(plot)

# Show with Bokeh


graph_renderer.node_renderer.glyph = Circle(size=15, fill_color=Spectral4[0])
graph_renderer.edge_renderer.glyph = MultiLine(line_color="edge_color", line_alpha=0.8, line_width=1)
plot.renderers.append(graph_renderer)
output_file("interactive_graphs.html")
show(plot)


### community colors with high adj ###

highPosAdjacency = pd.read_csv('highPosAdjacency.csv', index_col=0)
highAdj = networkx.from_pandas_adjacency(highPosAdjacency)

highAdj_communities
len(highAdj_communities)

gm,gn = [],[]
for i,j in enumerate(highAdj_communities):
    gn += [i]*len(j)
    gm += list(j)

gnum = pd.Series(gn, index=gm)
gnum.index.duplicated().any() ## no duplicate OTUs, that's good.

## now we need to embed this group membership as an attribute of a node:

for i in highAdj.nodes:
    highAdj.nodes[i]['comGroup'] = gnum[i] 

## we need some colors:

colmap = genXKCDColors(len(highAdj_communities))

node_attrs = {}
for node in highAdj.nodes(data=False):
    node_color = colmap.colors[highAdj.nodes[node]['comGroup']]
    node_attrs[node] = node_color

networkx.set_node_attributes(highAdj, node_attrs, "node_color")

## now use this color attribute with bokeh:


plot = figure(title="High Disease Network",
                x_range=(-1.1,1.1),
                y_range=(-1.1,1.1),
                )
#graph = from_networkx(highAdj, networkx.spring_layout, scale=2, center=(0,0)) ## not bad, kawai is better
#graph = from_networkx(highAdj, networkx.circular_layout, scale=2, center=(0,0)) ## nope
graph = from_networkx(highAdj, networkx.kamada_kawai_layout, scale=2, center=(0,0)) ## nice one
#graph = from_networkx(highAdj, networkx.planar_layout, scale=2, center=(0,0)) ## not possible
#graph = from_networkx(highAdj, networkx.random_layout, center=(0,0)) ## horrible, as expected
#graph = from_networkx(highAdj, networkx.shell_layout, scale=2, center=(0,0)) ## same as circle, sucks
#graph = from_networkx(highAdj, networkx.spectral_layout, scale=2, center=(0,0)) ## nope
#graph = from_networkx(highAdj, networkx.spiral_layout, scale=2, center=(0,0)) ## pretty and useless
#graph = from_networkx(highAdj, networkx.multipartite_layout, scale=2, center=(0,0)) ## doesn't work
graph.edge_renderer.glyph = MultiLine(line_color="#CCCCCC", line_alpha=0.4, line_width=2)
graph.node_renderer.glyph = Circle(size=15, fill_color="node_color")
plot.renderers.append(graph)
output_file("interactive_graphs.html")
show(plot)

## great. do both with the kamada_kawai style, in a notebook. 


## venn diagrams



## one way to measure the change in network structure is by 
## comparing how the networks changed in terms of high-degree
## nodes. 


venn2([
    set(highAdjDeg[0:200].index),
    set(lowAdjDeg[0:200].index)
])

## and how do our OTUs compare generally? How many OTUs are 
## shared between low and high disease samples?


## low
lowHiDiseaseTablePA = pd.read_csv('lowHiDiseaseTablePA.csv', index_col=0)
aa = lowHiDiseaseTablePA[lowHiDiseaseTablePA['lowDisease'] == 1]
(aa.sum(axis=0) == 0).any()
(aa.sum(axis=0) == 0).sum()
lowDiseaseAllColumns = aa.loc[:,~(aa.sum(axis=0) == 0)].columns

## high
lowHiDiseaseTablePA = pd.read_csv('lowHiDiseaseTablePA.csv', index_col=0)
aa = lowHiDiseaseTablePA[lowHiDiseaseTablePA['highDisease'] == 1]
(aa.sum(axis=0) == 0).any()
(aa.sum(axis=0) == 0).sum()
highDiseaseAllColumns = aa.loc[:,~(aa.sum(axis=0) == 0)].columns

venn2([
    set(highDiseaseAllColumns),
    set(lowDiseaseAllColumns)
])

## Though both graphs show similar general membership,
##  connectedness, clustering, their is a clear shift in the OTUs that 
## are providing the hubs for the network, and the communities/modules
## that are forming are different.  
